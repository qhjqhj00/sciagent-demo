[
  {
      "status": "success",
      "result": [
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "Agentic Reinforcement Learning for Search is Unsafe",
              "authors": [
                  {
                      "name": "Yushi Yang",
                      "orgs": [
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Shreyansh Padarha",
                      "orgs": [
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Andrew Lee",
                      "orgs": [
                          "Harvard University"
                      ]
                  },
                  {
                      "name": "Adam Mahdi",
                      "orgs": [
                          "University of Oxford"
                      ]
                  }
              ],
              "tldr": "This paper investigates the safety of agentic reinforcement learning (RL) search models, which inherit refusal behavior from instruction tuning but are vulnerable to attacks that lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query safety by 82.4%; the study highlights the need for safety-aware RL pipelines to optimize for safe search.",
              "arxiv_id": "2510.17431",
              "urls": [
                  "https://arxiv.org/pdf/2510.17431v1"
              ],
              "dates": [
                  "2025-10-20T11:19:37Z"
              ],
              "score": 98.75
          },
          {
              "sources": [
                  "metadata",
                  "roc"
              ],
              "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
              "authors": [
                  {
                      "name": "Guibin Zhang",
                      "orgs": [
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Hejia Geng",
                      "orgs": [
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Xiaohang Yu",
                      "orgs": [
                          "Imperial College London"
                      ]
                  },
                  {
                      "name": "Zhenfei Yin",
                      "orgs": [
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Zaibin Zhang",
                      "orgs": [
                          "Dalian University of Technology",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Zelin Tan",
                      "orgs": [
                          "Shanghai AI Laboratory",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Heng Zhou",
                      "orgs": [
                          "Shanghai AI Laboratory",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Zhongzhi Li",
                      "orgs": [
                          "University of Science and Technology of China"
                      ]
                  },
                  {
                      "name": "Xiangyuan Xue",
                      "orgs": [
                          "Shanghai AI Laboratory",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Yijiang Li",
                      "orgs": [
                          "University of California San Diego"
                      ]
                  },
                  {
                      "name": "Yifan Zhou",
                      "orgs": [
                          "Shanghai AI Laboratory",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Yang Chen",
                      "orgs": [
                          "Shanghai AI Laboratory",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Chen Zhang",
                      "orgs": [
                          "Shanghai AI Laboratory",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Yutao Fan",
                      "orgs": [
                          "Shanghai AI Laboratory",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Zihu Wang",
                      "orgs": [
                          "University of Bristol"
                      ]
                  },
                  {
                      "name": "Songtao Huang",
                      "orgs": [
                          "Fudan University",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Yue Liao",
                      "orgs": [
                          "National University of Singapore"
                      ]
                  },
                  {
                      "name": "Hongru Wang",
                      "orgs": [
                          "The Chinese University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Mengyue Yang",
                      "orgs": [
                          "University of Bristol"
                      ]
                  },
                  {
                      "name": "Heng Ji",
                      "orgs": [
                          "University of Illinois Urbana-Champaign"
                      ]
                  },
                  {
                      "name": "Michael Littman",
                      "orgs": [
                          "Brown University"
                      ]
                  },
                  {
                      "name": "Jun Wang",
                      "orgs": [
                          "University College London"
                      ]
                  },
                  {
                      "name": "Shuicheng Yan",
                      "orgs": [
                          "National University of Singapore"
                      ]
                  },
                  {
                      "name": "Philip Torr",
                      "orgs": [
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Lei Bai",
                      "orgs": [
                          "Shanghai AI Laboratory",
                          "University of Oxford"
                      ]
                  }
              ],
              "tldr": "This paper reviews agentic reinforcement learning (Agentic RL) for large language models (LLMs), addressing the gap in unifying LLMs as policy-optimized agents in sequential decision processes; it employs a capability-centered taxonomy and MDP/POMDP formalization, contributing a comprehensive survey of over five hundred works; it identifies challenges in scalability and trustworthiness for future agentic AI development.",
              "arxiv_id": "2509.02547",
              "urls": [
                  "https://arxiv.org/pdf/2509.02547v1"
              ],
              "dates": [
                  "2025-09-02T17:46:26Z"
              ],
              "score": 95.0
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning",
              "authors": [
                  {
                      "name": "Youtu-Agent Team",
                      "orgs": [
                          "∗"
                      ]
                  }
              ],
              "tldr": "This paper addresses the exploration-exploitation trade-off in agentic reinforcement learning for LLMs, proposing SPEAR, a curriculum-based self-imitation learning method that balances entropy progression for stable training, and demonstrates performance improvements of up to 20.7% on WebShop and 16.1% on ALFWorld over existing baselines.",
              "arxiv_id": "2509.22601",
              "urls": [
                  "https://arxiv.org/pdf/2509.22601v2"
              ],
              "dates": [
                  "2025-09-26T17:20:38Z"
              ],
              "score": 95.0
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Agentic Reinforcement Learning with Implicit Step Rewards",
              "authors": [
                  {
                      "name": "Xiaoqian Liu",
                      "orgs": [
                          "University of Chinese Academy of Sciences",
                          "Tongyi Lab"
                      ]
                  },
                  {
                      "name": "Ke Wang",
                      "orgs": [
                          "Tongyi Lab"
                      ]
                  },
                  {
                      "name": "Yuchuan Wu",
                      "orgs": [
                          "Tongyi Lab"
                      ]
                  },
                  {
                      "name": "Fei Huang",
                      "orgs": [
                          "Tongyi Lab"
                      ]
                  },
                  {
                      "name": "Yongbin Li",
                      "orgs": [
                          "Tongyi Lab"
                      ]
                  },
                  {
                      "name": "Junge Zhang",
                      "orgs": [
                          "Institute of Automation, Chinese Academy of Sciences"
                      ]
                  },
                  {
                      "name": "Jianbin Jiao",
                      "orgs": [
                          "University of Chinese Academy of Sciences"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of credit assignment in agentic reinforcement learning for large language models, aiming to improve training stability and efficiency in environments with sparse, unverifiable rewards; it introduces iStar, a method that jointly trains an implicit process reward model with the policy using trajectory-based DPO, achieving state-of-the-art results with 14% and 48% higher goal completion in self-chat and GPT-4o interactions respectively.",
              "arxiv_id": "2509.19199",
              "urls": [
                  "https://arxiv.org/pdf/2509.19199v3"
              ],
              "dates": [
                  "2025-09-23T16:15:42Z"
              ],
              "score": 95.0
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
              "authors": [
                  {
                      "name": "Dongfu Jiang",
                      "orgs": [
                          "University of Waterloo",
                          "Sea AI Lab"
                      ]
                  },
                  {
                      "name": "Yi Lu",
                      "orgs": [
                          "University of Toronto"
                      ]
                  },
                  {
                      "name": "Zhuofeng Li",
                      "orgs": [
                          "Shanghai University"
                      ]
                  },
                  {
                      "name": "Zhiheng Lyu",
                      "orgs": [
                          "University of Waterloo"
                      ]
                  },
                  {
                      "name": "Ping Nie",
                      "orgs": [
                          "HKUST"
                      ]
                  },
                  {
                      "name": "Haozhe Wang",
                      "orgs": [
                          "HKUST",
                          "M-A-P"
                      ]
                  },
                  {
                      "name": "Alex Su",
                      "orgs": [
                          "University of Waterloo"
                      ]
                  },
                  {
                      "name": "Hui Chen",
                      "orgs": [
                          "National University of Singapore"
                      ]
                  },
                  {
                      "name": "Kai Zou",
                      "orgs": [
                          "NetMind.AI"
                      ]
                  },
                  {
                      "name": "Chao Du",
                      "orgs": [
                          "University of Waterloo"
                      ]
                  },
                  {
                      "name": "Tianyu Pang",
                      "orgs": [
                          "University of Waterloo"
                      ]
                  },
                  {
                      "name": "Wenhu Chen",
                      "orgs": [
                          "University of Waterloo"
                      ]
                  }
              ],
              "tldr": "This paper addresses the limitations of existing Agentic Reinforcement Learning with Tool use (ARLT) frameworks by introducing VerlTool, a unified and modular framework that enables efficient, scalable, and multimodal agentic training with tool integration, achieving over 2× speedup and competitive performance across six ARLT domains.",
              "arxiv_id": "2509.01055",
              "urls": [
                  "https://arxiv.org/pdf/2509.01055v3"
              ],
              "dates": [
                  "2025-09-01T01:45:18Z"
              ],
              "score": 95.0
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "rStar2-Agent: Agentic Reasoning Technical Report",
              "authors": [
                  {
                      "name": "Ning Shang",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Yifei Liu",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Yi Zhu",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Li Lyna Zhang",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Weijiang Xu",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Xinyu Guan",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Buze Zhang",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Bingcheng Dong",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Xudong Zhou",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Bowen Zhang",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Ying Xin",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Ziming Miao",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Scarlett Li",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Fan Yang",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Mao Yang",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  }
              ],
              "tldr": "This paper introduces rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to address limitations of long CoT in complex problem-solving, by enabling autonomous tool use and reflection; it employs GRPO-RoC and efficient infrastructure to achieve state-of-the-art performance with 510 RL steps, reaching 80.6% pass@1 on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1; its findings highlight advanced cognitive behaviors and generalization to scientific and alignment tasks.",
              "arxiv_id": "2508.20722",
              "urls": [
                  "https://arxiv.org/pdf/2508.20722v1"
              ],
              "dates": [
                  "2025-08-28T12:45:25Z"
              ],
              "score": 95.0
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "Agentic Reinforced Policy Optimization",
              "authors": [
                  {
                      "name": "Guanting Dong",
                      "orgs": [
                          "Renmin University of China",
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Hangyu Mao",
                      "orgs": [
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Kai Ma",
                      "orgs": [
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Licheng Bao",
                      "orgs": [
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Yifei Chen",
                      "orgs": [
                          "Renmin University of China"
                      ]
                  },
                  {
                      "name": "Zhongyuan Wang",
                      "orgs": [
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Zhongxia Chen",
                      "orgs": [
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Jiazhen Du",
                      "orgs": [
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Huiyang Wang",
                      "orgs": [
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Fuzheng Zhang",
                      "orgs": [
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Guorui Zhou",
                      "orgs": [
                          "Kuaishou Technology"
                      ]
                  },
                  {
                      "name": "Yutao Zhu",
                      "orgs": [
                          "Renmin University of China"
                      ]
                  },
                  {
                      "name": "Ji-Rong Wen",
                      "orgs": [
                          "Renmin University of China"
                      ]
                  },
                  {
                      "name": "Zhicheng Dou",
                      "orgs": [
                          "Renmin University of China"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of training LLM-based agents for multi-turn tool interactions in agentic reinforcement learning, proposing ARPO which uses an entropy-based adaptive rollout mechanism and advantage attribution estimation to improve performance, achieving superior results across 13 benchmarks with half the tool-use budget of existing methods.",
              "arxiv_id": "2507.19849",
              "urls": [
                  "https://arxiv.org/pdf/2507.19849v1"
              ],
              "dates": [
                  "2025-07-26T07:53:11Z"
              ],
              "score": 95.0
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents",
              "authors": [
                  {
                      "name": "Yansong Ning",
                      "orgs": [
                          "The Hong Kong University of Science and Technology (Guangzhou)"
                      ]
                  },
                  {
                      "name": "Rui Liu",
                      "orgs": [
                          "Didichuxing Co. Ltd"
                      ]
                  },
                  {
                      "name": "Jun Wang",
                      "orgs": [
                          "Didichuxing Co. Ltd"
                      ]
                  },
                  {
                      "name": "Kai Chen",
                      "orgs": [
                          "Didichuxing Co. Ltd"
                      ]
                  },
                  {
                      "name": "Wei Li",
                      "orgs": [
                          "Didichuxing Co. Ltd"
                      ]
                  },
                  {
                      "name": "Jun Fang",
                      "orgs": [
                          "Didichuxing Co. Ltd"
                      ]
                  },
                  {
                      "name": "Kan Zheng",
                      "orgs": [
                          "Didichuxing Co. Ltd"
                      ]
                  },
                  {
                      "name": "Naiqiang Tan",
                      "orgs": [
                          "Didichuxing Co. Ltd"
                      ]
                  },
                  {
                      "name": "Hao Liu",
                      "orgs": [
                          "The Hong Kong University of Science and Technology (Guangzhou)"
                      ]
                  }
              ],
              "tldr": "This paper proposes DeepTravel, an end-to-end agentic reinforcement learning framework for autonomous travel planning agents, addressing the limitations of existing methods by enabling flexible and autonomous TP agent training in dynamic environments; it introduces a robust sandbox environment, hierarchical reward modeling, and reply-augmented reinforcement learning, demonstrating that small LLMs outperform state-of-the-art models in travel planning tasks.",
              "arxiv_id": "2509.21842",
              "urls": [
                  "https://arxiv.org/pdf/2509.21842v1"
              ],
              "dates": [
                  "2025-09-26T04:03:52Z"
              ],
              "score": 92.5
          },
          {
              "sources": [
                  "section"
              ],
              "title": "AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents",
              "authors": [
                  {
                      "name": "Renxi Wang",
                      "orgs": [
                          "Mohamed bin Zayed University of Artificial Intelligence"
                      ]
                  },
                  {
                      "name": "Rifo Ahmad Genadi",
                      "orgs": [
                          "Mohamed bin Zayed University of Artificial Intelligence"
                      ]
                  },
                  {
                      "name": "Bilal El Bouardi",
                      "orgs": [
                          "Mohamed bin Zayed University of Artificial Intelligence"
                      ]
                  },
                  {
                      "name": "Yongxin Wang",
                      "orgs": [
                          "Mohamed bin Zayed University of Artificial Intelligence"
                      ]
                  },
                  {
                      "name": "Fajri Koto",
                      "orgs": [
                          "Mohamed bin Zayed University of Artificial Intelligence"
                      ]
                  },
                  {
                      "name": "Zhengzhong Liu",
                      "orgs": [
                          "Mohamed bin Zayed University of Artificial Intelligence"
                      ]
                  },
                  {
                      "name": "Timothy Baldwin",
                      "orgs": [
                          "Mohamed bin Zayed University of Artificial Intelligence"
                      ]
                  },
                  {
                      "name": "Haonan Li",
                      "orgs": [
                          "Mohamed bin Zayed University of Artificial Intelligence"
                      ]
                  }
              ],
              "tldr": "This paper introduces AgentFly, a scalable and extensible reinforcement learning framework for LM agents, addressing the underexplored integration of RL with LM agents by improving multi-turn interaction handling, efficient rollout processes, and modular design; it contributes a decorator-based interface, asynchronous execution, and centralized resource management; empirical results show successful agent training across six tasks with enhanced flexibility and scalability.",
              "arxiv_id": "2507.14897",
              "urls": [
                  "https://arxiv.org/pdf/2507.14897v1"
              ],
              "dates": [
                  "2025-07-20T10:22:36Z"
              ],
              "score": 92.5
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use",
              "authors": [],
              "tldr": "This paper addresses the challenge of agentic tool use in LLMs during multi-turn interactions, aiming to improve agents' ability to iteratively understand user needs and invoke tools effectively; it introduces MUA-RL, a novel reinforcement learning framework integrating LLM-simulated users, which enables efficient communication and problem-solving; evaluations show MUA-RL-32B achieves 67.3, 45.4, 28.3, 28.4, and 82.5 on various benchmarks, outperforming or matching larger models.",
              "arxiv_id": "2508.18669",
              "urls": [
                  "https://arxiv.org/pdf/2508.18669v1"
              ],
              "dates": [
                  "2025-08-26T04:26:29Z"
              ],
              "score": 91.25
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward",
              "authors": [
                  {
                      "name": "Yong Deng",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Guoqing Wang",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Zhenzhe Ying",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Xiaofeng Wu",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Jinzhen Lin",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Wenwen Xiong",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Yuqin Dai",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Shuo Yang",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Zhanwei Zhang",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Qiwen Wang",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Yang Qin",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Yuan Wang",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Quanxing Zha",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Sunhao Dai",
                      "orgs": [
                          "Ant Group"
                      ]
                  },
                  {
                      "name": "Changhua Meng",
                      "orgs": [
                          "Ant Group"
                      ]
                  }
              ],
              "tldr": "This paper addresses the limitations of current agentic deep research systems by introducing Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained units, and proposes Atom-Searcher, a RL framework integrating Atomic Thought Rewards (ATR) and a curriculum-inspired reward schedule, achieving significant performance improvements over SOTA on seven benchmarks.",
              "arxiv_id": "2508.12800",
              "urls": [
                  "https://arxiv.org/pdf/2508.12800v3"
              ],
              "dates": [
                  "2025-08-18T10:23:10Z"
              ],
              "score": 91.25
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Synthetic Social Media Influence Experimentation via an Agentic Reinforcement Learning Large Language Model Bot",
              "authors": [
                  {
                      "name": "Bailu Jin",
                      "orgs": [
                          "Cranfield University"
                      ]
                  },
                  {
                      "name": "Weisi Guo",
                      "orgs": [
                          "Cranfield University"
                      ]
                  }
              ],
              "tldr": "This paper presents a simulated environment using agentic reinforcement learning and LLMs to study social media influence dynamics, addressing the ethical challenges and data scarcity in real-world experiments; it introduces a framework with RL-driven agents that generate posts and form opinions, contributing novel insights on action space constraints and self-observation for stable opinion leader generation; empirical results show the framework effectively replicates real-world discussion patterns, offering practical applications in policy-making and marketing.",
              "arxiv_id": "2411.19635",
              "urls": [
                  "https://arxiv.org/pdf/2411.19635v2"
              ],
              "dates": [
                  "2024-11-29T11:37:12Z"
              ],
              "score": 90.0
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents",
              "authors": [
                  {
                      "name": "Yifan Xu",
                      "orgs": [
                          "Tsinghua University"
                      ]
                  },
                  {
                      "name": "Xiao Liu",
                      "orgs": [
                          "Tsinghua University",
                          "Z.AI"
                      ]
                  },
                  {
                      "name": "Xinghan Liu",
                      "orgs": [
                          "Tsinghua University"
                      ]
                  },
                  {
                      "name": "Jiaqi Fu",
                      "orgs": [
                          "Tsinghua University"
                      ]
                  },
                  {
                      "name": "Hanchen Zhang",
                      "orgs": [
                          "Tsinghua University"
                      ]
                  },
                  {
                      "name": "Bohao Jing",
                      "orgs": [
                          "Z.AI"
                      ]
                  },
                  {
                      "name": "Shudan Zhang",
                      "orgs": [
                          "Tsinghua University"
                      ]
                  },
                  {
                      "name": "Yuting Wang",
                      "orgs": [
                          "Z.AI"
                      ]
                  },
                  {
                      "name": "Wenyi Zhao",
                      "orgs": [
                          "Z.AI"
                      ]
                  },
                  {
                      "name": "Yuxiao Dong",
                      "orgs": [
                          "Tsinghua University"
                      ]
                  }
              ],
              "tldr": "This paper presents MobileRL, an online agentic reinforcement learning framework for mobile GUI agents, addressing challenges in task difficulty distribution and sample efficiency; it introduces AdaGRPO with difficulty-adaptive positive replay, failure curriculum filtering, and shortest-path reward adjustment; MobileRL-9B achieves 80.2% success rate on AndroidWorld and 53.6% on AndroidLab, outperforming previous state-of-the-art results.",
              "arxiv_id": "2509.18119",
              "urls": [
                  "https://arxiv.org/pdf/2509.18119v2"
              ],
              "dates": [
                  "2025-09-10T13:09:27Z"
              ],
              "score": 88.75
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning",
              "authors": [],
              "tldr": "This paper introduces Deep-DxSearch, an end-to-end agentic RAG system trained with reinforcement learning to improve traceable diagnostic reasoning in medical settings, addressing knowledge gaps and hallucinations in medical LLMs by enhancing retrieval and reasoning traceability; it uses a large-scale medical retrieval corpus and tailored rewards to train an LLM as an agent, achieving higher diagnostic accuracy than GPT-4o and DeepSeek-R1 across common and rare diseases; results show significant performance gains and improved interpretability, supporting more reliable clinical diagnoses.",
              "arxiv_id": "2508.15746",
              "urls": [
                  "https://arxiv.org/pdf/2508.15746v1"
              ],
              "dates": [
                  "2025-08-21T17:42:47Z"
              ],
              "score": 88.75
          },
          {
              "sources": [
                  "metadata",
                  "roc",
                  "section"
              ],
              "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning",
              "authors": [
                  {
                      "name": "Joykirat Singh",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Raghav Magazine",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Yash Pandya",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  },
                  {
                      "name": "Akshay Nambi",
                      "orgs": [
                          "Microsoft Research"
                      ]
                  }
              ],
              "tldr": "This paper addresses the limitations of static knowledge in LLMs by introducing ARTIST, a reinforcement learning framework for agentic reasoning and tool integration, which enables models to autonomously use tools and environments for complex tasks, achieving up to 22% absolute improvement over base models in mathematical reasoning and doubling accuracy in multi-turn function calling.",
              "arxiv_id": "2505.01441",
              "urls": [
                  "https://arxiv.org/pdf/2505.01441v1"
              ],
              "dates": [
                  "2025-04-28T10:42:49Z"
              ],
              "score": 88.75
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning",
              "authors": [
                  {
                      "name": "Wenlin Zhang",
                      "orgs": [
                          "City University of Hong Kong",
                          "Hong Kong"
                      ]
                  },
                  {
                      "name": "Xiangyang Li",
                      "orgs": [
                          "Noah’s Ark Lab",
                          "Huawei",
                          "China"
                      ]
                  },
                  {
                      "name": "Kuicai Dong",
                      "orgs": [
                          "Noah’s Ark Lab",
                          "Huawei",
                          "Singapore"
                      ]
                  },
                  {
                      "name": "Yichao Wang",
                      "orgs": [
                          "Noah’s Ark Lab",
                          "Huawei",
                          "China"
                      ]
                  },
                  {
                      "name": "Pengyue Jia",
                      "orgs": [
                          "City University of Hong Kong",
                          "Hong Kong"
                      ]
                  },
                  {
                      "name": "Xiaopeng Li",
                      "orgs": [
                          "City University of Hong Kong",
                          "Hong Kong"
                      ]
                  },
                  {
                      "name": "Yingyi Zhang",
                      "orgs": [
                          "City University of Hong Kong",
                          "Hong Kong"
                      ]
                  },
                  {
                      "name": "Derong Xu",
                      "orgs": [
                          "City University of Hong Kong",
                          "Hong Kong"
                      ]
                  },
                  {
                      "name": "Zhaocheng Du",
                      "orgs": [
                          "Noah’s Ark Lab",
                          "Huawei",
                          "China"
                      ]
                  },
                  {
                      "name": "Huifeng Guo",
                      "orgs": [
                          "Noah’s Ark Lab",
                          "Huawei",
                          "China"
                      ]
                  },
                  {
                      "name": "Ruiming Tang",
                      "orgs": [
                          "Noah’s Ark Lab",
                          "Huawei",
                          "China"
                      ]
                  },
                  {
                      "name": "Xiangyu Zhao",
                      "orgs": [
                          "City University of Hong Kong",
                          "Hong Kong"
                      ]
                  }
              ],
              "tldr": "This paper addresses the limitations of outcome-supervised reinforcement learning in agentic RAG by proposing process-supervised RL through ReasonRAG, which uses MCTS and SPRE to generate a 5k-query RAG-ProGuide dataset, achieving superior performance with 13k steps compared to Search-R1's 90k queries and 270k steps.",
              "arxiv_id": "2505.14069",
              "urls": [
                  "https://arxiv.org/pdf/2505.14069v2"
              ],
              "dates": [
                  "2025-05-20T08:21:00Z"
              ],
              "score": 88.75
          },
          {
              "sources": [
                  "metadata",
                  "roc",
                  "section"
              ],
              "title": "Visual Agentic Reinforcement Fine-Tuning",
              "authors": [
                  {
                      "name": "Ziyu Liu",
                      "orgs": [
                          "Shanghai Jiaotong University",
                          "Shanghai Artificial Intelligence Laboratory"
                      ]
                  },
                  {
                      "name": "Yuhang Zang",
                      "orgs": [
                          "Shanghai Artificial Intelligence Laboratory"
                      ]
                  },
                  {
                      "name": "Yushan Zou",
                      "orgs": [
                          "Wuhan University"
                      ]
                  },
                  {
                      "name": "Zijian Liang",
                      "orgs": [
                          "Shanghai Jiaotong University",
                          "Shanghai Artificial Intelligence Laboratory"
                      ]
                  },
                  {
                      "name": "Xiaoyi Dong",
                      "orgs": [
                          "Shanghai Artificial Intelligence Laboratory",
                          "The Chinese University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Yuhang Cao",
                      "orgs": [
                          "Shanghai Artificial Intelligence Laboratory"
                      ]
                  },
                  {
                      "name": "Haodong Duan",
                      "orgs": [
                          "Shanghai Artificial Intelligence Laboratory"
                      ]
                  },
                  {
                      "name": "Dahua Lin",
                      "orgs": [
                          "Shanghai Artificial Intelligence Laboratory",
                          "The Chinese University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Jiaqi Wang",
                      "orgs": [
                          "Shanghai Artificial Intelligence Laboratory"
                      ]
                  }
              ],
              "tldr": "This paper introduces Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) to enhance Large Vision-Language Models' agentic capabilities in multi-modal reasoning and tool use, addressing the gap in multi-modal agentic benchmarks; it employs reward-driven training with verifiable rewards and GRPO algorithm, contributing a novel framework and the Multimodal Agentic Tool Bench (MAT); experiments show Visual-ARFT outperforms baselines by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, demonstrating strong generalization.",
              "arxiv_id": "2505.14246",
              "urls": [
                  "https://arxiv.org/pdf/2505.14246v1"
              ],
              "dates": [
                  "2025-05-20T11:59:25Z"
              ],
              "score": 88.75
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Agentic Episodic Control",
              "authors": [],
              "tldr": "This paper proposes Agentic Episodic Control (AEC), a novel architecture integrating reinforcement learning with large language models to enhance decision-making by improving data efficiency and generalizability, particularly in complex tasks like FindObj where it outperforms baselines by up to 76%, bridging numeric RL and symbolic reasoning for more adaptable agents.",
              "arxiv_id": "2506.01442",
              "urls": [
                  "https://arxiv.org/pdf/2506.01442v1"
              ],
              "dates": [
                  "2025-06-02T08:57:37Z"
              ],
              "score": 87.5
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs",
              "authors": [
                  {
                      "name": "Siyu Zhu",
                      "orgs": [
                          "LinkedIn Corporation"
                      ]
                  },
                  {
                      "name": "Yanbin Jiang",
                      "orgs": [
                          "LinkedIn Corporation"
                      ]
                  },
                  {
                      "name": "Hejian Sang",
                      "orgs": [
                          "LinkedIn Corporation"
                      ]
                  },
                  {
                      "name": "Shao Tang",
                      "orgs": [
                          "LinkedIn Corporation"
                      ]
                  },
                  {
                      "name": "Qingquan Song",
                      "orgs": [
                          "LinkedIn Corporation"
                      ]
                  },
                  {
                      "name": "Biao He",
                      "orgs": [
                          "LinkedIn Corporation"
                      ]
                  },
                  {
                      "name": "Rohit Jain",
                      "orgs": [
                          "LinkedIn Corporation"
                      ]
                  },
                  {
                      "name": "Zhipeng Wang",
                      "orgs": [
                          "LinkedIn Corporation"
                      ]
                  },
                  {
                      "name": "Alborz Geramifard",
                      "orgs": [
                          "LinkedIn Corporation"
                      ]
                  }
              ],
              "tldr": "This paper investigates agentic reinforcement learning with large language models on the TravelPlanner benchmark, aiming to improve efficiency and performance by leveraging reward shaping, finding that smaller models (8B) achieve competitive results with 56.9% final-pass rate using dense rewards, while larger models show higher variance, and demonstrating that agentic RL can enhance efficiency without sacrificing generalization across out-of-domain tasks.",
              "arxiv_id": "2509.25779",
              "urls": [
                  "https://arxiv.org/pdf/2509.25779v2"
              ],
              "dates": [
                  "2025-09-30T04:49:36Z"
              ],
              "score": 87.5
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering",
              "authors": [
                  {
                      "name": "Zexi Liu",
                      "orgs": [
                          "Shanghai Jiao Tong University"
                      ]
                  },
                  {
                      "name": "Jingyi Chai",
                      "orgs": [
                          "Shanghai Jiao Tong University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Xinyu Zhu",
                      "orgs": [
                          "Shanghai Jiao Tong University"
                      ]
                  },
                  {
                      "name": "Shuo Tang",
                      "orgs": [
                          "Shanghai Jiao Tong University"
                      ]
                  },
                  {
                      "name": "Rui Ye",
                      "orgs": [
                          "Shanghai Jiao Tong University"
                      ]
                  },
                  {
                      "name": "Bo Zhang",
                      "orgs": [
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Lei Bai",
                      "orgs": [
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Siheng Chen",
                      "orgs": [
                          "Shanghai Jiao Tong University"
                      ]
                  }
              ],
              "tldr": "This paper introduces a new paradigm for autonomous ML, learning-based agentic ML, where an LLM agent learns through interactive experimentation using online RL, proposing a framework with exploration-enriched fine-tuning, step-wise RL, and reward module, and demonstrates that a 7B-sized ML-Agent outperforms a 671B-sized agent despite training on only 9 tasks.",
              "arxiv_id": "2505.23723",
              "urls": [
                  "https://arxiv.org/pdf/2505.23723v1"
              ],
              "dates": [
                  "2025-05-29T17:54:44Z"
              ],
              "score": 87.5
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents",
              "authors": [
                  {
                      "name": "Xuan-Phi Nguyen",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Shrey Pandit",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Revanth Gangi Reddy",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Austin Xu",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Silvio Savarese",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Caiming Xiong",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Shafiq Joty",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of autonomous reasoning in single-agent systems for Deep Research (DR), aiming to enhance agentic capabilities through continual reinforcement learning (RL) while preserving reasoning ability; it proposes a synthetic data-driven RL framework with memory management and temporal advantage normalization, achieving 28.7% on the Humanity's Last Exam benchmark; the findings highlight the effectiveness of single-agent RL in complex tasks, offering insights for future agentic system design and training methodologies.",
              "arxiv_id": "2509.06283",
              "urls": [
                  "https://arxiv.org/pdf/2509.06283v2"
              ],
              "dates": [
                  "2025-09-08T02:07:09Z"
              ],
              "score": 87.5
          },
          {
              "sources": [
                  "section"
              ],
              "title": "L0: Reinforcement Learning to Become General Agents",
              "authors": [
                  {
                      "name": "Junjie Zhang",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  },
                  {
                      "name": "Jingyi Xi",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  },
                  {
                      "name": "Zhuoyang Song",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  },
                  {
                      "name": "Junyu Lu",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  },
                  {
                      "name": "Yuhua Ke",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  },
                  {
                      "name": "Ting Sun",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  },
                  {
                      "name": "Yukun Yang",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  },
                  {
                      "name": "Jiaxing Zhang",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  },
                  {
                      "name": "Songxin Zhang",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  },
                  {
                      "name": "Zejian Xie",
                      "orgs": [
                          "Lionrock AI Lab",
                          "China Merchants Research Institute of Advanced Technology"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of training large language models as autonomous agents for multi-turn, long-horizon tasks by introducing L0, a scalable end-to-end reinforcement learning framework that improves accuracy on factuality question-answering benchmarks from 30% to 80% on SimpleQA and 22% to 41% on HotpotQA, demonstrating the effectiveness of RLVR in enhancing problem-solving capabilities.",
              "arxiv_id": "2506.23667",
              "urls": [
                  "https://arxiv.org/pdf/2506.23667v1"
              ],
              "dates": [
                  "2025-06-30T09:44:32Z"
              ],
              "score": 87.5
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation",
              "authors": [
                  {
                      "name": "Pengxiang Li",
                      "orgs": [
                          "Beijing Institute of Technology",
                          "State Key Laboratory of General Artificial Intelligence, BIGAI"
                      ]
                  },
                  {
                      "name": "Zechen Hu",
                      "orgs": [
                          "DataCanvas"
                      ]
                  },
                  {
                      "name": "Zirui Shang",
                      "orgs": [
                          "Beijing Institute of Technology",
                          "State Key Laboratory of General Artificial Intelligence, BIGAI"
                      ]
                  },
                  {
                      "name": "Jingrong Wu",
                      "orgs": [
                          "DataCanvas"
                      ]
                  },
                  {
                      "name": "Yang Liu",
                      "orgs": [
                          "State Key Laboratory of General Artificial Intelligence, BIGAI"
                      ]
                  },
                  {
                      "name": "Hui Liu",
                      "orgs": [
                          "DataCanvas"
                      ]
                  },
                  {
                      "name": "Zhi Gao",
                      "orgs": [
                          "Beijing Institute of Technology",
                          "State Key Laboratory of General Artificial Intelligence, BIGAI"
                      ]
                  },
                  {
                      "name": "Chenrui Shi",
                      "orgs": [
                          "Beijing Institute of Technology",
                          "State Key Laboratory of General Artificial Intelligence, BIGAI"
                      ]
                  },
                  {
                      "name": "Bofei Zhang",
                      "orgs": [
                          "State Key Laboratory of General Artificial Intelligence, BIGAI"
                      ]
                  },
                  {
                      "name": "Zihao Zhang",
                      "orgs": [
                          "DataCanvas"
                      ]
                  },
                  {
                      "name": "Xiaochuan Shi",
                      "orgs": [
                          "DataCanvas"
                      ]
                  },
                  {
                      "name": "Zedong Yu",
                      "orgs": [
                          "State Key Laboratory of General Artificial Intelligence, BIGAI",
                          "Beijing University of Posts and Telecommunications"
                      ]
                  },
                  {
                      "name": "Yuwei Wu",
                      "orgs": [
                          "Beijing Institute of Technology",
                          "State Key Laboratory of General Artificial Intelligence, BIGAI",
                          "Shenzhen MSU-BIT University"
                      ]
                  },
                  {
                      "name": "Xinxiao Wu",
                      "orgs": [
                          "Beijing Institute of Technology",
                          "State Key Laboratory of General Artificial Intelligence, BIGAI",
                          "Shenzhen MSU-BIT University"
                      ]
                  },
                  {
                      "name": "Yunde Jia",
                      "orgs": [
                          "Shenzhen MSU-BIT University"
                      ]
                  },
                  {
                      "name": "Liuyu Xiang",
                      "orgs": [
                          "Beijing University of Posts and Telecommunications"
                      ]
                  },
                  {
                      "name": "Zhaofeng He",
                      "orgs": [
                          "Beijing University of Posts and Telecommunications"
                      ]
                  },
                  {
                      "name": "Qing Li",
                      "orgs": [
                          "State Key Laboratory of General Artificial Intelligence, BIGAI"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenges of efficient multi-turn reinforcement learning for GUI agents by proposing DART, a decoupled training framework that improves GPU utilization by 1.6x, training throughput by 1.9x, and environment utilization by 5.5x, achieving a 42.13% task success rate with a 14.61% absolute gain over the base model and 7.34% higher than open-source SOTA.",
              "arxiv_id": "2509.23866",
              "urls": [
                  "https://arxiv.org/pdf/2509.23866v1"
              ],
              "dates": [
                  "2025-09-28T13:19:20Z"
              ],
              "score": 86.25
          },
          {
              "sources": [
                  "introduction",
                  "metadata"
              ],
              "title": "Agentic Skill Discovery",
              "authors": [
                  {
                      "name": "Xufeng Zhao",
                      "orgs": [
                          "University of Hamburg"
                      ]
                  },
                  {
                      "name": "Cornelius Weber",
                      "orgs": [
                          "University of Hamburg"
                      ]
                  },
                  {
                      "name": "Stefan Wermter",
                      "orgs": [
                          "University of Hamburg"
                      ]
                  }
              ],
              "tldr": "This paper introduces Agentic Skill Discovery, addressing the challenge of acquiring diverse robotic skills without initial skill libraries by proposing a framework driven by LLMs to generate tasks and reinforce learning, contributing novel skill acquisition through LLM-guided reinforcement learning and vision-language model validation, demonstrating the emergence of meaningful skills from zero initial knowledge.",
              "arxiv_id": "2405.15019",
              "urls": [
                  "https://arxiv.org/pdf/2405.15019v2"
              ],
              "dates": [
                  "2024-05-23T19:44:03Z"
              ],
              "score": 86.25
          },
          {
              "sources": [
                  "section"
              ],
              "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning",
              "authors": [
                  {
                      "name": "Wenfeng Feng",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  },
                  {
                      "name": "Penghong Zhao",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  },
                  {
                      "name": "Guochao Jiang",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  },
                  {
                      "name": "Chuzhan Hao",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  },
                  {
                      "name": "Yuewei Zhang",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  },
                  {
                      "name": "Guohua Liu",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  },
                  {
                      "name": "Hao Wang",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  }
              ],
              "tldr": "This paper introduces PVPO, an efficient critic-free reinforcement learning method addressing the issue of local optima and high computational cost in grouping policies by using a reference anchor and data pre-sampling, contributing a stable advantage function and improved training efficiency, achieving state-of-the-art performance on multi-step retrieval datasets and enhancing LLM reasoning capabilities.",
              "arxiv_id": "2508.21104",
              "urls": [
                  "https://arxiv.org/pdf/2508.21104v3"
              ],
              "dates": [
                  "2025-08-28T09:18:26Z"
              ],
              "score": 85.0
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning",
              "authors": [
                  {
                      "name": "Hongyu Lin",
                      "orgs": [
                          "1",
                          "2"
                      ]
                  },
                  {
                      "name": "Yuchen Li",
                      "orgs": [
                          "1",
                          "2"
                      ]
                  },
                  {
                      "name": "Haoran Luo",
                      "orgs": [
                          "3"
                      ]
                  },
                  {
                      "name": "Kaichun Yao",
                      "orgs": [
                          "2"
                      ]
                  },
                  {
                      "name": "Libo Zhang",
                      "orgs": [
                          "2"
                      ]
                  },
                  {
                      "name": "Mingjie Xing",
                      "orgs": [
                          "2"
                      ]
                  },
                  {
                      "name": "Yanjun Wu",
                      "orgs": [
                          "1",
                          "2"
                      ]
                  }
              ],
              "tldr": "This paper presents OS-R1, an agentic Linux kernel tuning framework using rule-based reinforcement learning to address the challenges of efficient and scalable kernel configuration optimization, by abstracting the configuration space as an RL environment and designing custom reward functions, achieving up to 5.6% performance improvement over heuristic tuning and demonstrating strong cross-scenario generalization.",
              "arxiv_id": "2508.12551",
              "urls": [
                  "https://arxiv.org/pdf/2508.12551v1"
              ],
              "dates": [
                  "2025-08-18T01:09:57Z"
              ],
              "score": 83.75
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities",
              "authors": [
                  {
                      "name": "Rikuto Kotoge",
                      "orgs": [
                          "OMRON SINIC X Corporation",
                          "The University of Osaka"
                      ]
                  },
                  {
                      "name": "Mai Nishimura",
                      "orgs": [
                          "OMRON SINIC X Corporation"
                      ]
                  },
                  {
                      "name": "Jiaxin Ma",
                      "orgs": [
                          "OMRON SINIC X Corporation"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of enabling compact language models (0.5–1B parameters) to exhibit agentic RAG capabilities, which is important for resource-constrained environments; it proposes Distillation-Guided Policy Optimization (DGPO), combining cold-start initialization and selective teacher guidance to stabilize training and improve performance; experiments show DGPO outperforms baselines and achieves teacher-surpassing results on several datasets.",
              "arxiv_id": "2508.20324",
              "urls": [
                  "https://arxiv.org/pdf/2508.20324v3"
              ],
              "dates": [
                  "2025-08-27T23:57:29Z"
              ],
              "score": 83.75
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents",
              "authors": [
                  {
                      "name": "Ayesha Amjad",
                      "orgs": [
                          "1"
                      ]
                  },
                  {
                      "name": "Saurav Sthapit",
                      "orgs": [
                          "2"
                      ]
                  },
                  {
                      "name": "Tahir Qasim Syed",
                      "orgs": [
                          "1"
                      ]
                  }
              ],
              "tldr": "This paper proposes an agentic AI system using LLM agents and reinforcement learning to improve form-like document parsing, addressing limitations of monolithic approaches in handling diverse layouts and semantics; it introduces a modular multi-agent framework with RL-driven self-improvement, showing promising results on SOIRE and CORD datasets; findings suggest the framework enhances accuracy and adaptability in automated information extraction.",
              "arxiv_id": "2505.13504",
              "urls": [
                  "https://arxiv.org/pdf/2505.13504v1"
              ],
              "dates": [
                  "2025-05-16T09:46:10Z"
              ],
              "score": 82.5
          },
          {
              "sources": [
                  "section"
              ],
              "title": "From Correction to Mastery: Reinforced Distillation of Large Language Model Agents",
              "authors": [
                  {
                      "name": "Yuanjie Lyu",
                      "orgs": [
                          "University of Science and Technology of China"
                      ]
                  },
                  {
                      "name": "Chengyu Wang",
                      "orgs": [
                          "Independent Researcher"
                      ]
                  },
                  {
                      "name": "Jun Huang",
                      "orgs": [
                          "Independent Researcher"
                      ]
                  },
                  {
                      "name": "Tong Xu",
                      "orgs": [
                          "University of Science and Technology of China"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of distilling large language model agents into smaller models, aiming to reduce cost while preserving performance by mitigating compounding errors through student-centered correction and reinforcement learning; it introduces SCoRe, a framework that generates student trajectories with teacher correction at the earliest error and applies short-horizon reinforcement learning with key-step rewards; experiments show a 7B-parameter student matches the agentic performance of a 72B-parameter teacher on 12 benchmarks.",
              "arxiv_id": "2509.14257",
              "urls": [
                  "https://arxiv.org/pdf/2509.14257v2"
              ],
              "dates": [
                  "2025-09-12T15:34:07Z"
              ],
              "score": 82.5
          },
          {
              "sources": [
                  "roc"
              ],
              "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
              "authors": [
                  {
                      "name": "Yuxiang Zheng",
                      "orgs": [
                          "SJTU",
                          "SII",
                          "GAIR"
                      ]
                  },
                  {
                      "name": "Dayuan Fu",
                      "orgs": [
                          "SII",
                          "GAIR"
                      ]
                  },
                  {
                      "name": "Xiangkun Hu",
                      "orgs": [
                          "SII"
                      ]
                  },
                  {
                      "name": "Xiaojie Cai",
                      "orgs": [
                          "SJTU",
                          "SII",
                          "GAIR"
                      ]
                  },
                  {
                      "name": "Lyumanshan Ye",
                      "orgs": [
                          "SJTU",
                          "SII",
                          "GAIR"
                      ]
                  },
                  {
                      "name": "Pengrui Lu",
                      "orgs": [
                          "SJTU",
                          "SII",
                          "GAIR"
                      ]
                  },
                  {
                      "name": "Pengfei Liu",
                      "orgs": [
                          "SJTU",
                          "SII",
                          "GAIR"
                      ]
                  }
              ],
              "tldr": "This paper introduces DeepResearcher, a framework for training LLM-based research agents via reinforcement learning in real-world web environments to address limitations of prompt engineering and RAG-based methods, achieving up to 28.9 point improvements over prompt-based baselines and 7.2 points over RAG-based RL agents, with findings showing emergent cognitive behaviors like plan formulation and cross-validation.",
              "arxiv_id": "2504.03160",
              "urls": [
                  "https://arxiv.org/pdf/2504.03160v4"
              ],
              "dates": [
                  "2025-04-04T04:41:28Z"
              ],
              "score": 82.5
          },
          {
              "sources": [
                  "introduction",
                  "section"
              ],
              "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
              "authors": [
                  {
                      "name": "Kuan Li",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhongwang Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Huifeng Yin",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Rui Ye",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yida Zhao",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Liwen Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Litu Ou",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Dingchu Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Xixi Wu",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jialong Wu",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Xinyu Wang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zile Qiao",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhen Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yong Jiang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Pengjun Xie",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Fei Huang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jingren Zhou",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  }
              ],
              "tldr": "This paper addresses the gap in complex information-seeking tasks between open-source and proprietary agents by proposing WebSailor-V2, which introduces a synthetic data generation method and scalable reinforcement learning training, achieving 35.3 on BrowseComp-EN and 44.1 on BrowseComp-ZH, outperforming existing open-source agents and surpassing a 671B-sized proprietary model.",
              "arxiv_id": "2509.13305",
              "urls": [
                  "https://arxiv.org/pdf/2509.13305v1"
              ],
              "dates": [
                  "2025-09-16T17:57:03Z"
              ],
              "score": 81.25
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors",
              "authors": [
                  {
                      "name": "Alan Dao (Gia Tuan Dao)",
                      "orgs": [
                          "Menlo Research"
                      ]
                  },
                  {
                      "name": "Dinh Bach Vu",
                      "orgs": [
                          "Menlo Research"
                      ]
                  },
                  {
                      "name": "Alex Nguyen",
                      "orgs": [
                          "Menlo Research"
                      ]
                  },
                  {
                      "name": "Norapat Buppodom",
                      "orgs": [
                          "Menlo Research"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of stabilizing reasoning in agentic web search for small language models, proposing a dynamic task vector machine to enable coherent, self-constructed reasoning; it introduces RLVR optimization and MCP integration, achieving 78.3% accuracy on SimpleQA with a 1.7B-parameter model; findings show small models can rival larger ones through structured reasoning, challenging assumptions about model size and data scaling.",
              "arxiv_id": "2508.00360",
              "urls": [
                  "https://arxiv.org/pdf/2508.00360v1"
              ],
              "dates": [
                  "2025-08-01T06:45:29Z"
              ],
              "score": 81.25
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "Kimi K2: Open Agentic Intelligence",
              "authors": [
                  {
                      "name": "Kimi Team",
                      "orgs": []
                  }
              ],
              "tldr": "This paper introduces Kimi K2, a large language model designed to advance agentic intelligence by addressing challenges in training and post-training through MuonClip optimizer, agentic data synthesis, and reinforcement learning, achieving state-of-the-art performance in agentic and general tasks with scores of 66.1, 76.5, 65.8, and 47.3 on key benchmarks, positioning it as a top open-source model.",
              "arxiv_id": "2507.20534",
              "urls": [
                  "https://arxiv.org/pdf/2507.20534v1"
              ],
              "dates": [
                  "2025-07-28T05:35:43Z"
              ],
              "score": 81.25
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "AWorld: Orchestrating the Training Recipe for Agentic AI",
              "authors": [],
              "tldr": "This paper addresses the challenge of efficient experience generation in agentic AI development, introducing AWorld, an open-source system that accelerates agent-environment interaction by 14.6x, leading to a Qwen3-32B agent achieving 32.23% pass@1 accuracy on GAIA, surpassing GPT-4o and rivaling DeepSeek-V3, thereby offering a scalable training framework for agentic AI.",
              "arxiv_id": "2508.20404",
              "urls": [
                  "https://arxiv.org/pdf/2508.20404v2"
              ],
              "dates": [
                  "2025-08-28T04:04:30Z"
              ],
              "score": 80.0
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents",
              "authors": [
                  {
                      "name": "Weiting Tan",
                      "orgs": [
                          "Johns Hopkins University"
                      ]
                  },
                  {
                      "name": "Xinghua Qu",
                      "orgs": [
                          "ByteDance"
                      ]
                  },
                  {
                      "name": "Ming Tu",
                      "orgs": []
                  },
                  {
                      "name": "Meng Ge",
                      "orgs": []
                  },
                  {
                      "name": "Andy T. Liu",
                      "orgs": []
                  },
                  {
                      "name": "Philipp Koehn",
                      "orgs": []
                  },
                  {
                      "name": "Lu Lu",
                      "orgs": []
                  }
              ],
              "tldr": "This paper addresses the challenge of training interactive tool-use agents in multi-modal environments by introducing TARL, a reinforcement learning framework that uses an LLM as a turn-level judge to improve credit assignment and exploration, achieving a 6% increase in task pass rate on text-based tasks and over 20% improvement on multimodal tasks, demonstrating its effectiveness for developing voice-driven agentic systems.",
              "arxiv_id": "2509.14480",
              "urls": [
                  "https://arxiv.org/pdf/2509.14480v1"
              ],
              "dates": [
                  "2025-09-17T23:25:00Z"
              ],
              "score": 80.0
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards",
              "authors": [
                  {
                      "name": "Jeff Da",
                      "orgs": [
                          "Scale AI"
                      ]
                  },
                  {
                      "name": "Clinton Wang",
                      "orgs": [
                          "Scale AI"
                      ]
                  },
                  {
                      "name": "Xiang Deng",
                      "orgs": [
                          "Scale AI"
                      ]
                  },
                  {
                      "name": "Yuntao Ma",
                      "orgs": [
                          "Scale AI"
                      ]
                  },
                  {
                      "name": "Nikhil Barhate",
                      "orgs": [
                          "Scale AI"
                      ]
                  },
                  {
                      "name": "Sean Hendryx",
                      "orgs": [
                          "Scale AI"
                      ]
                  }
              ],
              "tldr": "This paper introduces Agent-RLVR, a framework addressing sparse reward challenges in agentic settings by integrating agent guidance to improve software engineering agent performance, achieving a 22.4% pass@1 on SWE-Bench Verified, with further improvements to 27.8% when using guidance-augmented RLVR data for reward model training.",
              "arxiv_id": "2506.11425",
              "urls": [
                  "https://arxiv.org/pdf/2506.11425v2"
              ],
              "dates": [
                  "2025-06-13T02:46:53Z"
              ],
              "score": 80.0
          },
          {
              "sources": [
                  "introduction",
                  "metadata"
              ],
              "title": "Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library",
              "authors": [
                  {
                      "name": "ROLL Team",
                      "orgs": [
                          "Alibaba"
                      ]
                  }
              ],
              "tldr": "This paper introduces ROLL, a library for efficient and scalable reinforcement learning optimization in large-scale learning, addressing the challenge of complex multi-stage training pipelines for LLMs by providing a modular architecture with single-controller design, optimized parallel strategies, and fine-grained rollout scheduling; its contributions include flexible resource allocation, fault-tolerant training, and support for agentic RL; results show successful training of 200B+ MoE models on thousands of GPUs and validation on multi-domain RLVR and agentic tasks.",
              "arxiv_id": "2506.06122",
              "urls": [
                  "https://arxiv.org/pdf/2506.06122v1"
              ],
              "dates": [
                  "2025-06-06T14:33:56Z"
              ],
              "score": 80.0
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning",
              "authors": [
                  {
                      "name": "Jinyeop Song",
                      "orgs": [
                          "MIT"
                      ]
                  },
                  {
                      "name": "Song Wang",
                      "orgs": [
                          "University of Central Florida"
                      ]
                  },
                  {
                      "name": "Julian Shun",
                      "orgs": [
                          "MIT"
                      ]
                  },
                  {
                      "name": "Yada Zhu",
                      "orgs": [
                          "MIT-IBM Watson AI Lab",
                          "IBM Research"
                      ]
                  }
              ],
              "tldr": "This paper introduces KG-R1, an agentic KG-RAG framework addressing the inefficiency and lack of transferability in multi-module KG-RAG systems by using reinforcement learning to optimize a single-agent workflow, achieving improved answer accuracy with fewer generation tokens and maintaining strong performance across new KGs without modification.",
              "arxiv_id": "2509.26383",
              "urls": [
                  "https://arxiv.org/pdf/2509.26383v3"
              ],
              "dates": [
                  "2025-09-30T15:14:24Z"
              ],
              "score": 78.75
          },
          {
              "sources": [
                  "section"
              ],
              "title": "DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning",
              "authors": [
                  {
                      "name": "Chuzhan Hao",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  },
                  {
                      "name": "Wenfeng Feng",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  },
                  {
                      "name": "Yuewei Zhang",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  },
                  {
                      "name": "Hao Wang",
                      "orgs": [
                          "Alibaba Cloud Computing"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of generating factually consistent intermediate queries and efficient search trajectories in multi-step agentic retrieval systems, proposing DynaSearcher, a dynamic knowledge graph augmented search agent using multi-reward reinforcement learning; it introduces knowledge graphs for structured knowledge guidance and a multi-reward framework to balance accuracy, efficiency, and response quality; experimental results show state-of-the-art performance on six multi-hop QA datasets with strong generalization and robustness.",
              "arxiv_id": "2507.17365",
              "urls": [
                  "https://arxiv.org/pdf/2507.17365v2"
              ],
              "dates": [
                  "2025-07-23T09:58:31Z"
              ],
              "score": 78.75
          },
          {
              "sources": [
                  "introduction"
              ],
              "title": "Can Large Reasoning Models Self-Train?",
              "authors": [
                  {
                      "name": "Sheikh Shafayat",
                      "orgs": [
                          "1*"
                      ]
                  },
                  {
                      "name": "Fahim Tajwar",
                      "orgs": [
                          "2*"
                      ]
                  },
                  {
                      "name": "Ruslan Salakhutdinov",
                      "orgs": [
                          "2"
                      ]
                  },
                  {
                      "name": "Jeff Schneider",
                      "orgs": [
                          "2"
                      ]
                  },
                  {
                      "name": "Andrea Zanette",
                      "orgs": [
                          "2"
                      ]
                  },
                  {
                      "name": "Independent Researcher",
                      "orgs": []
                  },
                  {
                      "name": "Carnegie Mellon University",
                      "orgs": []
                  }
              ],
              "tldr": "This paper investigates self-training in large reasoning models, addressing the challenge of scalable model improvement without human feedback, by proposing SRT which uses model self-consistency as a reward signal; it shows early performance comparable to gold-standard methods but reveals reward hacking risks leading to performance collapse; results highlight the need for better feedback mechanisms to sustain self-improvement.",
              "arxiv_id": "2505.21444",
              "urls": [
                  "https://arxiv.org/pdf/2505.21444v2"
              ],
              "dates": [
                  "2025-05-27T17:16:00Z"
              ],
              "score": 78.75
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach",
              "authors": [
                  {
                      "name": "Yinqiu Liu",
                      "orgs": [
                          "College of Computing and Data Science, Nanyang Technological University, Singapore"
                      ]
                  },
                  {
                      "name": "Guangyuan Liu",
                      "orgs": [
                          "College of Computing and Data Science, Nanyang Technological University, Singapore"
                      ]
                  },
                  {
                      "name": "Jiacheng Wang",
                      "orgs": [
                          "College of Computing and Data Science, Nanyang Technological University, Singapore"
                      ]
                  },
                  {
                      "name": "Ruichen Zhang",
                      "orgs": [
                          "College of Computing and Data Science, Nanyang Technological University, Singapore"
                      ]
                  },
                  {
                      "name": "Dusit Niyato",
                      "orgs": [
                          "College of Computing and Data Science, Nanyang Technological University, Singapore"
                      ]
                  },
                  {
                      "name": "Geng Sun",
                      "orgs": [
                          "College of Computer Science and Technology, Jilin University, China",
                          "College of Computing and Data Science, Nanyang Technological University, Singapore"
                      ]
                  },
                  {
                      "name": "Zehui Xiong",
                      "orgs": [
                          "Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Singapore"
                      ]
                  },
                  {
                      "name": "Zhu Han",
                      "orgs": [
                          "Department of Electrical and Computer Engineering, University of Houston, Houston, USA",
                          "Department of Computer Science and Engineering, Kyung Hee University, South Korea"
                      ]
                  }
              ],
              "tldr": "This paper addresses intent-aware agentic network optimization in the context of Generative AI agents, aiming to enhance Quality-of-Experience (QoE) by dynamically adapting to user intents; it introduces LAMeTA, combining Intent-oriented Knowledge Distillation (IoKD) and Symbiotic Reinforcement Learning (SRL) to distill intent understanding and optimize service function chains; experiments with 81 agents show IoKD reduces intent prediction error by 22.5% and SRL improves QoE by up to 23.5% over generic DRL.",
              "arxiv_id": "2505.12247",
              "urls": [
                  "https://arxiv.org/pdf/2505.12247v1"
              ],
              "dates": [
                  "2025-05-18T05:59:16Z"
              ],
              "score": 77.5
          },
          {
              "sources": [
                  "metadata",
                  "section"
              ],
              "title": "Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning",
              "authors": [
                  {
                      "name": "Haoran Luo",
                      "orgs": [
                          "Beijing University of Posts and Telecommunications",
                          "Nanyang Technological University"
                      ]
                  },
                  {
                      "name": "Haihong E",
                      "orgs": [
                          "Beijing University of Posts and Telecommunications"
                      ]
                  },
                  {
                      "name": "Guanting Chen",
                      "orgs": [
                          "Beijing University of Posts and Telecommunications"
                      ]
                  },
                  {
                      "name": "Qika Lin",
                      "orgs": [
                          "National University of Singapore"
                      ]
                  },
                  {
                      "name": "Yikai Guo",
                      "orgs": [
                          "Beijing Institute of Computer Technology and Application"
                      ]
                  },
                  {
                      "name": "Fangzhi Xu",
                      "orgs": [
                          "Nanyang Technological University"
                      ]
                  },
                  {
                      "name": "Zemin Kuang",
                      "orgs": [
                          "Beijing Anzhen Hospital, Capital Medical University"
                      ]
                  },
                  {
                      "name": "Meina Song",
                      "orgs": [
                          "Beijing University of Posts and Telecommunications"
                      ]
                  },
                  {
                      "name": "Xiaobao Wu",
                      "orgs": [
                          "Nanyang Technological University"
                      ]
                  },
                  {
                      "name": "Yifan Zhu",
                      "orgs": [
                          "Beijing University of Posts and Telecommunications"
                      ]
                  },
                  {
                      "name": "Luu Anh Tuan",
                      "orgs": [
                          "Nanyang Technological University"
                      ]
                  }
              ],
              "tldr": "This paper proposes Graph-R1, an agentic GraphRAG framework addressing RAG's limitations in structural semantics, retrieval efficiency, and generation quality through end-to-end reinforcement learning, outperforming existing methods in reasoning accuracy, retrieval efficiency, and generation quality by 12.3% on standard RAG datasets.",
              "arxiv_id": "2507.21892",
              "urls": [
                  "https://arxiv.org/pdf/2507.21892v1"
              ],
              "dates": [
                  "2025-07-29T15:01:26Z"
              ],
              "score": 77.5
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning",
              "authors": [
                  {
                      "name": "Lang Mei",
                      "orgs": [
                          "Huawei Cloud BU"
                      ]
                  },
                  {
                      "name": "Zhihan Yang",
                      "orgs": [
                          "Huawei Cloud BU"
                      ]
                  },
                  {
                      "name": "Chong Chen",
                      "orgs": [
                          "Huawei Cloud BU"
                      ]
                  }
              ],
              "tldr": "This paper proposes AI-SearchPlanner, a reinforcement learning framework addressing the limitation of single-LLM search agents by decoupling search planning and QA tasks, introducing dual-reward alignment and Pareto optimization, and demonstrating improved effectiveness and efficiency over existing methods across diverse datasets.",
              "arxiv_id": "2508.20368",
              "urls": [
                  "https://arxiv.org/pdf/2508.20368v3"
              ],
              "dates": [
                  "2025-08-28T02:31:17Z"
              ],
              "score": 77.5
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "SSRL: Self-Search Reinforcement Learning",
              "authors": [
                  {
                      "name": "Yuchen Fan",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Kaiyan Zhang",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Heng Zhou",
                      "orgs": [
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Yuxin Zuo",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Yanxu Chen",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Yu Fu",
                      "orgs": [
                          "University College London"
                      ]
                  },
                  {
                      "name": "Xinwei Long",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Xuekai Zhu",
                      "orgs": [
                          "Shanghai Jiao Tong University"
                      ]
                  },
                  {
                      "name": "Che Jiang",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Yuchen Zhang",
                      "orgs": [
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Li Kang",
                      "orgs": [
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Gang Chen",
                      "orgs": [
                          "CSCEC Third Bureau"
                      ]
                  },
                  {
                      "name": "Cheng Huang",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Zhizhou He",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Bingning Wang",
                      "orgs": [
                          "WeChat AI"
                      ]
                  },
                  {
                      "name": "Lei Bai",
                      "orgs": [
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Ning Ding",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Bowen Zhou",
                      "orgs": [
                          "Tsinghua University",
                          "Shanghai AI Laboratory"
                      ]
                  }
              ],
              "tldr": "This paper explores the use of large language models (LLMs) as simulators for agentic search tasks in reinforcement learning, addressing the challenge of costly external search interactions; it introduces Self-Search RL (SSRL), which enhances LLMs' internal search capabilities through format-based and rule-based rewards, achieving high pass@k scores on benchmarks; findings show SSRL outperforms existing methods, reduces hallucination, and enables robust sim-to-real transfer.",
              "arxiv_id": "2508.10874",
              "urls": [
                  "https://arxiv.org/pdf/2508.10874v1"
              ],
              "dates": [
                  "2025-08-14T17:46:01Z"
              ],
              "score": 77.5
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Primitive Agentic First-Order Optimization",
              "authors": [],
              "tldr": "This paper proposes a primitive agentic first-order optimization framework using reinforcement learning to address the challenge of efficient resource allocation in budget-limited optimization tasks, introducing a novel approach that combines agent-environment interactions with partial state representations; it contributes a method that outperforms conventional algorithms on quadratic optimization problems by 12.3% in efficiency, demonstrating the potential of agentic strategies to enhance computational rationality and reduce environmental impact.",
              "arxiv_id": "2406.04841",
              "urls": [
                  "https://arxiv.org/pdf/2406.04841v1"
              ],
              "dates": [
                  "2024-06-07T11:13:38Z"
              ],
              "score": 76.25
          },
          {
              "sources": [
                  "roc"
              ],
              "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning",
              "authors": [
                  {
                      "name": "Zhenghai Xue",
                      "orgs": [
                          "Nanyang Technological University, Singapore",
                          "TikTok, Singapore"
                      ]
                  },
                  {
                      "name": "Longtao Zheng",
                      "orgs": [
                          "Nanyang Technological University, Singapore",
                          "TikTok, Singapore"
                      ]
                  },
                  {
                      "name": "Qian Liu",
                      "orgs": [
                          "Nanyang Technological University, Singapore",
                          "TikTok, Singapore"
                      ]
                  },
                  {
                      "name": "Yingru Li",
                      "orgs": [
                          "Nanyang Technological University, Singapore",
                          "TikTok, Singapore"
                      ]
                  },
                  {
                      "name": "Xiaosen Zheng",
                      "orgs": [
                          "Nanyang Technological University, Singapore",
                          "TikTok, Singapore"
                      ]
                  },
                  {
                      "name": "Zejun Ma",
                      "orgs": [
                          "Nanyang Technological University, Singapore",
                          "TikTok, Singapore"
                      ]
                  },
                  {
                      "name": "Bo An",
                      "orgs": [
                          "Nanyang Technological University, Singapore",
                          "TikTok, Singapore"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of multi-turn Tool-Integrated Reasoning (TIR) training instability in Large Language Models (LLMs) by introducing SimpleTIR, a trajectory filtering method that removes void turns, leading to improved performance on math reasoning benchmarks with an AIME24 score increase from 22.1 to 50.5, and enabling diverse reasoning strategies without supervised fine-tuning.",
              "arxiv_id": "2509.02479",
              "urls": [
                  "https://arxiv.org/pdf/2509.02479v2"
              ],
              "dates": [
                  "2025-09-02T16:30:19Z"
              ],
              "score": 75.0
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Emergent Agentic Transformer from Chain of Hindsight Experience",
              "authors": [
                  {
                      "name": "Hao Liu",
                      "orgs": [
                          "University of California, Berkeley"
                      ]
                  },
                  {
                      "name": "Pieter Abbeel",
                      "orgs": [
                          "University of California, Berkeley"
                      ]
                  }
              ],
              "tldr": "This paper addresses the limitation of transformer-based policies in reinforcement learning by proposing Agentic Transformer (AT), which uses chain of hindsight experience to improve learning from sub-optimal data, achieving competitive performance on D4RL and ExoRL benchmarks with scaling trends in model size and chain length.",
              "arxiv_id": "2305.16554",
              "urls": [
                  "https://arxiv.org/pdf/2305.16554v1"
              ],
              "dates": [
                  "2023-05-26T00:43:02Z"
              ],
              "score": 75.0
          },
          {
              "sources": [
                  "introduction",
                  "section"
              ],
              "title": "Policy Search, Retrieval, and Composition via Task Similarity in Collaborative Agentic Systems",
              "authors": [
                  {
                      "name": "Saptarshi Nath",
                      "orgs": [
                          "Loughborough University"
                      ]
                  },
                  {
                      "name": "Christos Peridis",
                      "orgs": [
                          "Loughborough University"
                      ]
                  },
                  {
                      "name": "Eseoghene Benjamin",
                      "orgs": [
                          "Alan Turing Institute"
                      ]
                  },
                  {
                      "name": "Xinran Liu",
                      "orgs": [
                          "Vanderbilt University"
                      ]
                  },
                  {
                      "name": "Soheil Kolouri",
                      "orgs": [
                          "Vanderbilt University"
                      ]
                  },
                  {
                      "name": "Peter Kinnell",
                      "orgs": [
                          "Loughborough University"
                      ]
                  },
                  {
                      "name": "Zexin Li",
                      "orgs": [
                          "University of California Riverside"
                      ]
                  },
                  {
                      "name": "Cong Liu",
                      "orgs": [
                          "University of California Riverside"
                      ]
                  },
                  {
                      "name": "Shirin Dora",
                      "orgs": [
                          "Loughborough University"
                      ]
                  },
                  {
                      "name": "Andrea Soltoggio",
                      "orgs": [
                          "Loughborough University"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of knowledge sharing and reuse in agentic systems to improve learning efficiency, proposing MOSAIC which uses Wasserstein task embeddings, cosine similarity, and modular masks for policy selection and integration, leading to faster learning and solving tasks isolated agents cannot.",
              "arxiv_id": "2506.05577",
              "urls": [
                  "https://arxiv.org/pdf/2506.05577v2"
              ],
              "dates": [
                  "2025-06-05T20:38:11Z"
              ],
              "score": 75.0
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning",
              "authors": [
                  {
                      "name": "Wenchuan Zhang",
                      "orgs": [
                          "1",
                          "2"
                      ]
                  },
                  {
                      "name": "Jingru Guo",
                      "orgs": [
                          "3"
                      ]
                  },
                  {
                      "name": "Hengzhe Zhang",
                      "orgs": [
                          "4"
                      ]
                  },
                  {
                      "name": "Penghao Zhang",
                      "orgs": [
                          "5"
                      ]
                  },
                  {
                      "name": "Jie Chen",
                      "orgs": [
                          "2"
                      ]
                  },
                  {
                      "name": "Shuwan Zhang",
                      "orgs": [
                          "6"
                      ]
                  },
                  {
                      "name": "Zhang Zhang",
                      "orgs": [
                          "1"
                      ]
                  },
                  {
                      "name": "Yuhao Yi",
                      "orgs": [
                          "1",
                          "2"
                      ]
                  },
                  {
                      "name": "Hong Bu",
                      "orgs": [
                          "1",
                          "2"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of hallucinations in pathology VLMs by proposing Patho-AgenticRAG, a multimodal RAG framework with image-text retrieval and reinforcement learning, which improves diagnostic accuracy and interpretability, outperforming existing models in complex tasks like multiple-choice diagnosis and visual question answering.",
              "arxiv_id": "2508.02258",
              "urls": [
                  "https://arxiv.org/pdf/2508.02258v2"
              ],
              "dates": [
                  "2025-08-04T10:03:08Z"
              ],
              "score": 75.0
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Generative to Agentic AI: Survey, Conceptualization, and Challenges",
              "authors": [
                  {
                      "name": "Johannes Schneider",
                      "orgs": [
                          "University of Liechtenstein"
                      ]
                  }
              ],
              "tldr": "This paper examines the evolution from Generative AI to Agentic AI, addressing the gap in understanding their distinctions and implications, by comparing their capabilities, highlighting Agentic AI's enhanced reasoning and interaction through reinforcement learning, and reporting that Agentic AI outperforms GenAI in tasks like ARC challenge and radiology reports but shows only minor improvements on MMLU, with implications for future research, policy, and ethical considerations.",
              "arxiv_id": "2504.18875",
              "urls": [
                  "https://arxiv.org/pdf/2504.18875v1"
              ],
              "dates": [
                  "2025-04-26T09:47:00Z"
              ],
              "score": 75.0
          },
          {
              "sources": [
                  "roc"
              ],
              "title": "Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning",
              "authors": [
                  {
                      "name": "Yaorui Shi",
                      "orgs": [
                          "University of Science and Technology of China"
                      ]
                  },
                  {
                      "name": "Sihang Li",
                      "orgs": [
                          "University of Science and Technology of China"
                      ]
                  },
                  {
                      "name": "Chang Wu",
                      "orgs": [
                          "University of Science and Technology of China"
                      ]
                  },
                  {
                      "name": "Zhiyuan Liu",
                      "orgs": [
                          "National University of Singapore"
                      ]
                  },
                  {
                      "name": "Junfeng Fang",
                      "orgs": [
                          "National University of Singapore"
                      ]
                  },
                  {
                      "name": "Hengxing Cai",
                      "orgs": [
                          "DP Technology"
                      ]
                  },
                  {
                      "name": "An Zhang",
                      "orgs": [
                          "University of Science and Technology of China"
                      ]
                  },
                  {
                      "name": "Xiang Wang",
                      "orgs": [
                          "University of Science and Technology of China"
                      ]
                  }
              ],
              "tldr": "This paper addresses the limitations of retrieval-augmented reasoning by introducing AutoRefine, a reinforcement learning framework that incorporates search-and-refine-during-think paradigm and retrieval-specific rewards, achieving 6.9% higher average accuracy in multi-hop QA benchmarks compared to existing methods.",
              "arxiv_id": "2505.11277",
              "urls": [
                  "https://arxiv.org/pdf/2505.11277v5"
              ],
              "dates": [
                  "2025-05-16T14:11:29Z"
              ],
              "score": 75.0
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning",
              "authors": [
                  {
                      "name": "Yushi Feng",
                      "orgs": [
                          "1"
                      ]
                  },
                  {
                      "name": "Junye Du",
                      "orgs": [
                          "1"
                      ]
                  },
                  {
                      "name": "Yingying Hong",
                      "orgs": [
                          "1"
                      ]
                  },
                  {
                      "name": "Qifan Wang",
                      "orgs": [
                          "2"
                      ]
                  },
                  {
                      "name": "Lequan Yu",
                      "orgs": [
                          "1"
                      ]
                  }
              ],
              "tldr": "This paper addresses the limitations of existing agentic systems in medical CXR reasoning by introducing PASS, a probabilistic agentic supernet framework that enables interpretable, adaptive, and multimodal reasoning through task-conditioned distribution sampling over a multi-tool graph; it contributes a three-stage training strategy and CAB-E benchmark with 2,550 cases, showing improved accuracy and cost efficiency in complex CXR tasks.",
              "arxiv_id": "2508.10501",
              "urls": [
                  "https://arxiv.org/pdf/2508.10501v2"
              ],
              "dates": [
                  "2025-08-14T10:03:47Z"
              ],
              "score": 73.75
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories",
              "authors": [
                  {
                      "name": "Nanxu Gong",
                      "orgs": [
                          "Arizona State University"
                      ]
                  },
                  {
                      "name": "Sixun Dong",
                      "orgs": [
                          "Arizona State University"
                      ]
                  },
                  {
                      "name": "Haoyue Bai",
                      "orgs": [
                          "Arizona State University"
                      ]
                  },
                  {
                      "name": "Xinyuan Wang",
                      "orgs": [
                          "Arizona State University"
                      ]
                  },
                  {
                      "name": "Wangyang Ying",
                      "orgs": [
                          "Arizona State University"
                      ]
                  },
                  {
                      "name": "Yanjie Fu",
                      "orgs": [
                          "Arizona State University"
                      ]
                  }
              ],
              "tldr": "This paper addresses the gap in feature engineering by unifying feature selection and generation through an agentic framework, proposing MAGS with selector, generator, and router agents to optimize feature sets; it introduces long and short-term memory and offline PPO reinforcement learning for efficient and effective feature augmentation, demonstrating superior performance in navigating large discrete feature spaces.",
              "arxiv_id": "2505.15076",
              "urls": [
                  "https://arxiv.org/pdf/2505.15076v1"
              ],
              "dates": [
                  "2025-05-21T03:49:24Z"
              ],
              "score": 72.5
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy",
              "authors": [],
              "tldr": "This paper introduces ASTREA, an agentic system for spacecraft thermal control, addressing the challenge of integrating AI in resource-constrained space hardware; it combines a resource-constrained LLM agent with reinforcement learning in a hybrid architecture, offering scalable agentic supervision; on-orbit experiments aboard the ISS showed reduced violations, extended episode durations, and improved CPU utilization.",
              "arxiv_id": "2509.13380",
              "urls": [
                  "https://arxiv.org/pdf/2509.13380v2"
              ],
              "dates": [
                  "2025-09-16T08:52:13Z"
              ],
              "score": 72.5
          },
          {
              "sources": [
                  "roc"
              ],
              "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
              "authors": [],
              "tldr": "This paper addresses the gap in open-source models' ability to handle extreme uncertainty in complex information-seeking tasks, introducing WebSailor, a post-training methodology using structured sampling, information obfuscation, and DUPO algorithm, which outperforms open-source agents and matches proprietary agents' performance.",
              "arxiv_id": "2507.02592",
              "urls": [
                  "https://arxiv.org/pdf/2507.02592v1"
              ],
              "dates": [
                  "2025-07-03T12:59:07Z"
              ],
              "score": 72.5
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Natural Language Reinforcement Learning",
              "authors": [],
              "tldr": "This paper introduces Natural Language Reinforcement Learning (NLRL), a framework that addresses limitations in traditional RL by using a Language Value Function (LVF) to enable interpretable linguistic narratives, enhancing agent understanding and active learning through LLMs, with experiments showing its effectiveness across 4 multi-step tasks.",
              "arxiv_id": "2411.14251",
              "urls": [
                  "https://arxiv.org/pdf/2411.14251v3"
              ],
              "dates": [
                  "2024-11-21T15:57:02Z"
              ],
              "score": 72.5
          },
          {
              "sources": [
                  "introduction"
              ],
              "title": "EvolveSearch: An Iterative Self-Evolving Search Agent",
              "authors": [
                  {
                      "name": "Dingchu Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yida Zhao",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jialong Wu",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Baixuan Li",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Wenbiao Yin",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Liwen Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yong Jiang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yufeng Li",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Kewei Tu",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Pengjun Xie",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Fei Huang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  }
              ],
              "tldr": "This paper proposes EvolveSearch, a novel iterative self-evolution framework combining SFT and RL to enhance web search capabilities without human-annotated data, addressing the limitations of SFT in open domains and RL's low data efficiency; it achieves a 4.7% average accuracy improvement over state-of-the-art on seven MHQA benchmarks, demonstrating the effectiveness of self-evolving agentic systems.",
              "arxiv_id": "2505.22501",
              "urls": [
                  "https://arxiv.org/pdf/2505.22501v1"
              ],
              "dates": [
                  "2025-05-28T15:50:48Z"
              ],
              "score": 72.5
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Towards General Agentic Intelligence via Environment Scaling",
              "authors": [
                  {
                      "name": "Runnan Fang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Shihao Cai",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Baixuan Li",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jialong Wu",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Guangyu Li",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Wenbiao Yin",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Xinyu Wang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Xiaobin Wang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Liangcai Su",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhen Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Shibin Wu",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhengwei Tao",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yong Jiang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Pengjun Xie",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Fei Huang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jingren Zhou",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of enhancing function-calling capabilities of language agents through environment scaling, aiming to bridge the gap in agentic data scarcity by creating diverse simulated environments; it introduces a two-stage framework for environment construction and agent fine-tuning, leading to the development of AgentScaler models that achieve state-of-the-art performance with fewer parameters.",
              "arxiv_id": "2509.13311",
              "urls": [
                  "https://arxiv.org/pdf/2509.13311v1"
              ],
              "dates": [
                  "2025-09-16T17:57:20Z"
              ],
              "score": 71.25
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning",
              "authors": [
                  {
                      "name": "Zelin Tan",
                      "orgs": [
                          "University of Science and Technology of China",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Hejia Geng",
                      "orgs": [
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Mulei Zhang",
                      "orgs": [
                          "Wuhan University"
                      ]
                  },
                  {
                      "name": "Xiaohang Yu",
                      "orgs": [
                          "Imperial College London"
                      ]
                  },
                  {
                      "name": "Guancheng Wan",
                      "orgs": [
                          "Wuhan University"
                      ]
                  },
                  {
                      "name": "Yifan Zhou",
                      "orgs": [
                          "University of Georgia"
                      ]
                  },
                  {
                      "name": "Qiang He",
                      "orgs": [
                          "Chinese Academy of Sciences"
                      ]
                  },
                  {
                      "name": "Xiangyuan Xue",
                      "orgs": [
                          "University of Science and Technology of China",
                          "The Chinese University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Heng Zhou",
                      "orgs": [
                          "University of Science and Technology of China",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Yutao Fan",
                      "orgs": [
                          "University of Science and Technology of China",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Zhong-Zhi Li",
                      "orgs": [
                          "Chinese Academy of Sciences"
                      ]
                  },
                  {
                      "name": "Zaibin Zhang",
                      "orgs": [
                          "Dalian University of Technology",
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Guibin Zhang",
                      "orgs": [
                          "National University of Singapore"
                      ]
                  },
                  {
                      "name": "Chen Zhang",
                      "orgs": [
                          "University of Science and Technology of China",
                          "Shanghai AI Laboratory"
                      ]
                  },
                  {
                      "name": "Zhenfei Yin",
                      "orgs": [
                          "University of Oxford"
                      ]
                  }
              ],
              "tldr": "This paper investigates the scaling behaviors of LLMs in reinforcement learning post-training for mathematical reasoning, addressing the gap in understanding how model size, data volume, and computational budget affect performance; it employs empirical experiments across 54 models and finds that larger models outperform smaller ones under fixed compute or data constraints, with data reuse proving effective; results show consistent scaling dynamics across model types and hyperparameters, offering practical guidelines for efficient resource allocation in RL post-training.",
              "arxiv_id": "2509.25300",
              "urls": [
                  "https://arxiv.org/pdf/2509.25300v1"
              ],
              "dates": [
                  "2025-09-29T17:10:35Z"
              ],
              "score": 71.25
          },
          {
              "sources": [
                  "introduction",
                  "roc",
                  "section"
              ],
              "title": "WebDancer: Towards Autonomous Information Seeking Agency",
              "authors": [
                  {
                      "name": "Jialong Wu",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Baixuan Li",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Runnan Fang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Wenbiao Yin",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Liwen Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhengwei Tao",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Dingchu Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zekun Xi",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Gang Fu",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yong Jiang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Pengjun Xie",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Fei Huang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jingren Zhou",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  }
              ],
              "tldr": "This paper introduces WebDancer, an autonomous information seeking agent addressing the challenge of building capable web agents through a four-stage pipeline: browsing data construction, trajectory sampling, supervised fine-tuning for cold start, and reinforcement learning for generalization; it proposes a systematic approach combining rejection sampling fine-tuning with on-policy RL and achieves strong performance on GAIA and WebWalkerQA benchmarks, demonstrating effective training paradigms for real-world web tasks.",
              "arxiv_id": "2505.22648",
              "urls": [
                  "https://arxiv.org/pdf/2505.22648v3"
              ],
              "dates": [
                  "2025-05-28T17:57:07Z"
              ],
              "score": 71.25
          },
          {
              "sources": [
                  "roc"
              ],
              "title": "MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning",
              "authors": [
                  {
                      "name": "Kuo Yang",
                      "orgs": [
                          "University of Science and Technology of China (USTC)"
                      ]
                  },
                  {
                      "name": "Xingjie Yang",
                      "orgs": [
                          "University of Science and Technology of China (USTC)"
                      ]
                  },
                  {
                      "name": "Linhui Yu",
                      "orgs": [
                          "University of Science and Technology of China (USTC)"
                      ]
                  },
                  {
                      "name": "Qing Xu",
                      "orgs": [
                          "University of Science and Technology of China (USTC)"
                      ]
                  },
                  {
                      "name": "Yan Fang",
                      "orgs": [
                          "University of Science and Technology of China (USTC)",
                          "Hong Kong University of Science and Technology (Guangzhou)"
                      ]
                  },
                  {
                      "name": "Xu Wang",
                      "orgs": [
                          "University of Science and Technology of China (USTC)",
                          "Suzhou Institute for Advanced Research USTC"
                      ]
                  },
                  {
                      "name": "Zhengyang Zhou",
                      "orgs": [
                          "University of Science and Technology of China (USTC)",
                          "Suzhou Institute for Advanced Research USTC"
                      ]
                  },
                  {
                      "name": "Yang Wang",
                      "orgs": [
                          "University of Science and Technology of China (USTC)",
                          "Suzhou Institute for Advanced Research USTC"
                      ]
                  }
              ],
              "tldr": "This paper proposes MasHost, an RL-based framework for autonomous multi-agent system design, addressing the limitations of manual and semi-autonomous methods by enabling fully query-adaptive and rational Mas construction; it introduces a joint probabilistic sampling mechanism and Hierarchical Relative Policy Optimization (HRPO) to achieve high performance, efficiency, and structure rationality, with experiments showing MasHost outperforms baselines across six benchmarks.",
              "arxiv_id": "2506.08507",
              "urls": [
                  "https://arxiv.org/pdf/2506.08507v2"
              ],
              "dates": [
                  "2025-06-10T07:04:25Z"
              ],
              "score": 71.25
          },
          {
              "sources": [
                  "introduction",
                  "section"
              ],
              "title": "Scaling Agents via Continual Pre-training",
              "authors": [
                  {
                      "name": "Liangcai Su",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhen Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Guangyu Li",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhuo Chen",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Chenxi Wang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Maojia Song",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Xinyu Wang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Kuan Li",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jialong Wu",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Xuanzhong Chen",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zile Qiao",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhongwang Zhang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Huifeng Yin",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Shihao Cai",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Runnan Fang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhengwei Tao",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Wenbiao Yin",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Chenxiong Qian",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yong Jiang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Pengjun Xie",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Fei Huang",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jingren Zhou",
                      "orgs": [
                          "Tongyi Lab",
                          "Alibaba Group"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of training agentic systems for complex problem-solving by proposing Agentic Continual Pre-training (Agentic CPT) to build robust agentic foundation models, achieving state-of-the-art results with AgentFounder-30B on benchmarks like BrowseComp-en (39.9%) and HLE (31.5%), demonstrating effective scaling and strong tool-use capabilities.",
              "arxiv_id": "2509.13310",
              "urls": [
                  "https://arxiv.org/pdf/2509.13310v1"
              ],
              "dates": [
                  "2025-09-16T17:57:19Z"
              ],
              "score": 67.5
          },
          {
              "sources": [
                  "roc",
                  "section"
              ],
              "title": "Agentic Large Language Models, a survey",
              "authors": [
                  {
                      "name": "Aske Plaat",
                      "orgs": [
                          "Leiden University"
                      ]
                  },
                  {
                      "name": "Max van Duijn",
                      "orgs": [
                          "Leiden University"
                      ]
                  },
                  {
                      "name": "Niki van Stein",
                      "orgs": [
                          "Leiden University"
                      ]
                  },
                  {
                      "name": "Mike Preuss",
                      "orgs": [
                          "Leiden University"
                      ]
                  },
                  {
                      "name": "Peter van der Putten",
                      "orgs": [
                          "Leiden University"
                      ]
                  },
                  {
                      "name": "Kees Joost Batenburg",
                      "orgs": [
                          "Leiden University"
                      ]
                  }
              ],
              "tldr": "This paper reviews agentic LLMs, which reason, act, and interact, addressing the challenge of improving decision-making and enabling real-world applications; it uses a survey approach to categorize works into reasoning, acting, and interacting, highlighting their mutual benefits and contributions to data generation and LLM training; findings show agentic LLMs are applied in medicine, logistics, and finance, with potential to generate new training data and enhance inference, offering implications for future research and practical implementations.",
              "arxiv_id": "2503.23037",
              "urls": [
                  "https://arxiv.org/pdf/2503.23037v2"
              ],
              "dates": [
                  "2025-03-29T11:02:20Z"
              ],
              "score": 67.5
          },
          {
              "sources": [
                  "roc"
              ],
              "title": "Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges",
              "authors": [
                  {
                      "name": "Jintao Liang",
                      "orgs": [
                          "Beijing University of Posts and Telecommunications"
                      ]
                  },
                  {
                      "name": "Gang Su",
                      "orgs": [
                          "University of Georgia"
                      ]
                  },
                  {
                      "name": "Huifeng Lin",
                      "orgs": [
                          "South China University of Technology"
                      ]
                  },
                  {
                      "name": "You Wu",
                      "orgs": [
                          "South China University of Technology"
                      ]
                  },
                  {
                      "name": "Rui Zhao",
                      "orgs": [
                          "SenseTime Research",
                          "Qingyuan Research Institute",
                          "Shanghai Jiaotong University"
                      ]
                  },
                  {
                      "name": "Ziyue Li",
                      "orgs": [
                          "Technical University of Munich",
                          "University of Cologne"
                      ]
                  }
              ],
              "tldr": "This paper reviews Reasoning Agentic RAG systems, addressing the limitations of traditional RAG in complex industrial tasks by categorizing methods into predefined and agentic reasoning, which enhance flexibility and adaptability through structured pipelines and autonomous tool use, respectively, with implications for improving multi-modal and dynamic reasoning in real-world applications.",
              "arxiv_id": "2506.10408",
              "urls": [
                  "https://arxiv.org/pdf/2506.10408v1"
              ],
              "dates": [
                  "2025-06-12T07:01:56Z"
              ],
              "score": 66.25
          },
          {
              "sources": [
                  "introduction",
                  "section"
              ],
              "title": "Digi-Q: Learning Q-Value Functions for Training Device-Control Agents",
              "authors": [
                  {
                      "name": "Hao Bai",
                      "orgs": [
                          "UC Berkeley",
                          "UIUC"
                      ]
                  },
                  {
                      "name": "Yifei Zhou",
                      "orgs": [
                          "UC Berkeley"
                      ]
                  },
                  {
                      "name": "Li Erran Li",
                      "orgs": [
                          "Amazon"
                      ]
                  },
                  {
                      "name": "Sergey Levine",
                      "orgs": [
                          "UC Berkeley"
                      ]
                  },
                  {
                      "name": "Aviral Kumar",
                      "orgs": [
                          "Carnegie Mellon University"
                      ]
                  }
              ],
              "tldr": "This paper proposes Digi-Q, an offline reinforcement learning approach that trains VLM-based Q-functions for device-control agents, addressing the challenge of training policies in dynamic environments with limited interaction by using frozen VLM features and fine-tuning for actionable information, achieving 21.2% improvement over prior methods in Android device control tasks.",
              "arxiv_id": "2502.15760",
              "urls": [
                  "https://arxiv.org/pdf/2502.15760v1"
              ],
              "dates": [
                  "2025-02-13T18:55:14Z"
              ],
              "score": 65.0
          },
          {
              "sources": [
                  "section"
              ],
              "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents",
              "authors": [
                  {
                      "name": "Weizhi Zhang",
                      "orgs": [
                          "University of Illinois Chicago"
                      ]
                  },
                  {
                      "name": "Yangning Li",
                      "orgs": [
                          "Tsinghua University"
                      ]
                  },
                  {
                      "name": "Yuanchen Bei",
                      "orgs": [
                          "University of Illinois Urbana-Champaign"
                      ]
                  },
                  {
                      "name": "Junyu Luo",
                      "orgs": []
                  },
                  {
                      "name": "Guancheng Wan",
                      "orgs": [
                          "University of California, Los Angeles"
                      ]
                  },
                  {
                      "name": "Liangwei Yang",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Chenxuan Xie",
                      "orgs": [
                          "Zhejiang University of Technology"
                      ]
                  },
                  {
                      "name": "Yuyao Yang",
                      "orgs": [
                          "University of Illinois Chicago"
                      ]
                  },
                  {
                      "name": "Wei-Chieh Huang",
                      "orgs": [
                          "University of Illinois Chicago"
                      ]
                  },
                  {
                      "name": "Chunyu Miao",
                      "orgs": [
                          "University of Illinois Chicago"
                      ]
                  },
                  {
                      "name": "Henry Peng Zou",
                      "orgs": [
                          "University of Illinois Chicago"
                      ]
                  },
                  {
                      "name": "Xiao Luo",
                      "orgs": [
                          "University of California, Los Angeles"
                      ]
                  },
                  {
                      "name": "Yusheng Zhao",
                      "orgs": [
                          "Peking University"
                      ]
                  },
                  {
                      "name": "Yankai Chen",
                      "orgs": [
                          "University of Illinois Chicago"
                      ]
                  },
                  {
                      "name": "Chunkit Chan",
                      "orgs": [
                          "The Hong Kong University of Science and Technology"
                      ]
                  },
                  {
                      "name": "Peilin Zhou",
                      "orgs": [
                          "The Hong Kong University of Science and Technology (Guangzhou)"
                      ]
                  },
                  {
                      "name": "Xinyang Zhang",
                      "orgs": [
                          "Amazon"
                      ]
                  },
                  {
                      "name": "Chenwei Zhang",
                      "orgs": [
                          "Amazon"
                      ]
                  },
                  {
                      "name": "Jingbo Shang",
                      "orgs": [
                          "University of California, San Diego"
                      ]
                  },
                  {
                      "name": "Ming Zhang",
                      "orgs": [
                          "Peking University"
                      ]
                  },
                  {
                      "name": "Yangqiu Song",
                      "orgs": [
                          "The Hong Kong University of Science and Technology"
                      ]
                  },
                  {
                      "name": "Irwin King",
                      "orgs": [
                          "The Chinese University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Philip S. Yu",
                      "orgs": [
                          "University of Illinois Chicago"
                      ]
                  }
              ],
              "tldr": "This paper addresses the limitations of traditional keyword-based search engines in handling complex queries by introducing Agentic Deep Research, which integrates reasoning, iterative retrieval, and synthesis, and demonstrates its superior performance through test-time scaling laws and benchmark evaluations.",
              "arxiv_id": "2506.18959",
              "urls": [
                  "https://arxiv.org/pdf/2506.18959v3"
              ],
              "dates": [
                  "2025-06-23T17:27:19Z"
              ],
              "score": 65.0
          },
          {
              "sources": [
                  "introduction"
              ],
              "title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options",
              "authors": [
                  {
                      "name": "Lakshmi Nair",
                      "orgs": [
                          "1"
                      ]
                  },
                  {
                      "name": "Ian Trase",
                      "orgs": [
                          "1"
                      ]
                  },
                  {
                      "name": "J. Mark Kim",
                      "orgs": [
                          "1"
                      ]
                  }
              ],
              "tldr": "This paper proposes Flow-of-Options (FoO), a method to enhance LLM reasoning by exploring diverse solutions, addressing pre-training biases in ML tasks; FoO uses a network structure to enumerate options for each task step, offering novel agentic framework with improved performance over baselines; empirical results show 38.2%-69.2% and 37.4%-47.9% rank improvements on data science and therapeutic chemistry tasks, respectively, with costs under $1 per task.",
              "arxiv_id": "2502.12929",
              "urls": [
                  "https://arxiv.org/pdf/2502.12929v2"
              ],
              "dates": [
                  "2025-02-18T15:11:46Z"
              ],
              "score": 65.0
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search",
              "authors": [
                  {
                      "name": "Xin Lai",
                      "orgs": [
                          "ByteDance",
                          "The University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Junyi Li",
                      "orgs": [
                          "ByteDance",
                          "The University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Wei Li",
                      "orgs": [
                          "ByteDance",
                          "The University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Tao Liu",
                      "orgs": [
                          "ByteDance",
                          "The University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Tianjian Li",
                      "orgs": [
                          "ByteDance",
                          "The University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Hengshuang Zhao",
                      "orgs": [
                          "ByteDance",
                          "The University of Hong Kong"
                      ]
                  }
              ],
              "tldr": "This paper addresses the limitations of existing open-source VLMs in handling challenging visual search tasks by introducing Mini-o3, which scales up reasoning patterns and interaction turns through a Visual Probe Dataset, iterative data collection, and over-turn masking, achieving state-of-the-art performance with accuracy improving as interaction turns increase from 4 to 32.",
              "arxiv_id": "2509.07969",
              "urls": [
                  "https://arxiv.org/pdf/2509.07969v1"
              ],
              "dates": [
                  "2025-09-09T17:54:21Z"
              ],
              "score": 63.75
          },
          {
              "sources": [
                  "introduction",
                  "metadata"
              ],
              "title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?",
              "authors": [
                  {
                      "name": "Guibin Zhang",
                      "orgs": [
                          "NUS"
                      ]
                  },
                  {
                      "name": "Junhao Wang",
                      "orgs": [
                          "NUS"
                      ]
                  },
                  {
                      "name": "Junjie Chen",
                      "orgs": [
                          "NUS"
                      ]
                  },
                  {
                      "name": "Wangchunshu Zhou",
                      "orgs": [
                          "CUHK",
                          "OPPO",
                          "NTU"
                      ]
                  },
                  {
                      "name": "Kun Wang",
                      "orgs": [
                          "NUS"
                      ]
                  },
                  {
                      "name": "Shuicheng Yan",
                      "orgs": [
                          "NUS"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of identifying failure sources in LLM agentic systems, where current methods achieve below 10% accuracy, by proposing AgenTracer, an automated framework that uses counterfactual replay and programmatic fault injection to generate a large dataset and train AgenTracer-8B, which outperforms leading LLMs by up to 18.18% on Who&When and improves agentic system performance by 4.8-14.2%.",
              "arxiv_id": "2509.03312",
              "urls": [
                  "https://arxiv.org/pdf/2509.03312v2"
              ],
              "dates": [
                  "2025-09-03T13:42:14Z"
              ],
              "score": 63.75
          },
          {
              "sources": [
                  "introduction"
              ],
              "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs",
              "authors": [
                  {
                      "name": "Ranjan Sapkota",
                      "orgs": [
                          "Cornell University, Department of Biological and Environmental Engineering, Ithaca, NY 14853, USA"
                      ]
                  },
                  {
                      "name": "Konstantinos I. Roumeliotis",
                      "orgs": [
                          "Department of Informatics and Telecommunications, University of the Peloponnese, Tripoli, Greece"
                      ]
                  },
                  {
                      "name": "Manoj Karkee",
                      "orgs": [
                          "Cornell University, Department of Biological and Environmental Engineering, Ithaca, NY 14853, USA"
                      ]
                  }
              ],
              "tldr": "This survey reviews agentic UAVs in autonomous aerial intelligence, covering seven domains and highlighting technical challenges, AI advancements, and future directions for sustainable deployment.",
              "arxiv_id": "2506.08045",
              "urls": [
                  "https://arxiv.org/pdf/2506.08045v1"
              ],
              "dates": [
                  "2025-06-08T01:39:51Z"
              ],
              "score": 63.75
          },
          {
              "sources": [
                  "roc"
              ],
              "title": "Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning",
              "authors": [
                  {
                      "name": "Xuanyu Lei",
                      "orgs": [
                          "Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China",
                          "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China"
                      ]
                  },
                  {
                      "name": "Chenliang Li",
                      "orgs": [
                          "Tongyi Lab, Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yuning Wu",
                      "orgs": [
                          "Tongyi Lab, Alibaba Group"
                      ]
                  },
                  {
                      "name": "Kaiming Liu",
                      "orgs": [
                          "Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China",
                          "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China"
                      ]
                  },
                  {
                      "name": "Weizhou Shen",
                      "orgs": [
                          "Tongyi Lab, Alibaba Group"
                      ]
                  },
                  {
                      "name": "Peng Li",
                      "orgs": [
                          "Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China",
                          "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China"
                      ]
                  },
                  {
                      "name": "Ming Yan",
                      "orgs": [
                          "Tongyi Lab, Alibaba Group"
                      ]
                  },
                  {
                      "name": "Ji Zhang",
                      "orgs": [
                          "Tongyi Lab, Alibaba Group"
                      ]
                  },
                  {
                      "name": "Fei Huang",
                      "orgs": [
                          "Tongyi Lab, Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yang Liu",
                      "orgs": [
                          "Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China",
                          "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China"
                      ]
                  }
              ],
              "tldr": "This paper presents Writing-RL, an Adaptive Curriculum Reinforcement Learning framework for long-form writing that addresses limitations of supervised fine-tuning by improving data selection, reward design, and curriculum scheduling; the framework achieves state-of-the-art performance on 7B-scale writer models and shows generalization to long-input reasoning tasks.",
              "arxiv_id": "2506.05760",
              "urls": [
                  "https://arxiv.org/pdf/2506.05760v1"
              ],
              "dates": [
                  "2025-06-06T05:40:39Z"
              ],
              "score": 63.75
          },
          {
              "sources": [
                  "roc"
              ],
              "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making",
              "authors": [
                  {
                      "name": "Jake Grigsby",
                      "orgs": [
                          "The University of Texas at Austin",
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Yuke Zhu",
                      "orgs": [
                          "The University of Texas at Austin",
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Michael Ryoo",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  },
                  {
                      "name": "Juan Carlos Niebles",
                      "orgs": [
                          "Salesforce AI Research"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of aligning vision-language models (VLMs) with decision-making objectives in interactive environments, aiming to improve their performance on agent tasks where they lag behind LLMs in syntax and long-context prompting; it introduces an off-policy reinforcement learning approach that combines the stability of supervised fine-tuning with the self-improvement capabilities of RL, demonstrating its effectiveness across three domains with two open-weight VLMs.",
              "arxiv_id": "2505.03181",
              "urls": [
                  "https://arxiv.org/pdf/2505.03181v1"
              ],
              "dates": [
                  "2025-05-06T04:51:57Z"
              ],
              "score": 63.75
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle",
              "authors": [
                  {
                      "name": "Keliang Liu",
                      "orgs": [
                          "Fudan University",
                          "Shanghai",
                          "China"
                      ]
                  },
                  {
                      "name": "Dingkang Yang",
                      "orgs": [
                          "Fudan University",
                          "Shanghai",
                          "China",
                          "ByteDance SAIL Team",
                          "Shanghai",
                          "China"
                      ]
                  },
                  {
                      "name": "Ziyun Qian",
                      "orgs": [
                          "Fudan University",
                          "Shanghai",
                          "China"
                      ]
                  },
                  {
                      "name": "Weijie Yin",
                      "orgs": [
                          "ByteDance SAIL Team",
                          "Shanghai",
                          "China"
                      ]
                  },
                  {
                      "name": "Yuchi Wang",
                      "orgs": [
                          "The Chinese University of Hong Kong",
                          "Hongkong",
                          "China"
                      ]
                  },
                  {
                      "name": "Hongsheng Li",
                      "orgs": [
                          "The Chinese University of Hong Kong",
                          "MMLab",
                          "Hongkong",
                          "China"
                      ]
                  },
                  {
                      "name": "Jun Liu",
                      "orgs": [
                          "Lancaster University",
                          "Lancaster",
                          "UK"
                      ]
                  },
                  {
                      "name": "Peng Zhai",
                      "orgs": [
                          "Fudan University",
                          "Shanghai",
                          "China"
                      ]
                  },
                  {
                      "name": "Yang Liu",
                      "orgs": [
                          "Tongji University",
                          "Shanghai",
                          "China",
                          "The University of Toronto",
                          "Toronto",
                          "Canada"
                      ]
                  },
                  {
                      "name": "Lihua Zhang",
                      "orgs": [
                          "Fudan University",
                          "Shanghai",
                          "China"
                      ]
                  }
              ],
              "tldr": "This paper reviews RL-enhanced LLMs, addressing the challenge of aligning models with human preferences and improving reasoning capabilities through RL techniques; it employs a lifecycle-based approach, emphasizing RLVR, and contributes by systematically analyzing RL applications across pre-training, alignment, and reasoning; findings show RL significantly improves performance on benchmarks like MATH-500 and MMLU, with models like DeepSeek-R1 and OpenAI-o1 demonstrating substantial gains.",
              "arxiv_id": "2509.16679",
              "urls": [
                  "https://arxiv.org/pdf/2509.16679v1"
              ],
              "dates": [
                  "2025-09-20T13:11:28Z"
              ],
              "score": 62.5
          },
          {
              "sources": [
                  "section"
              ],
              "title": "Agent-as-a-Judge: Evaluate Agents with Agents",
              "authors": [
                  {
                      "name": "Mingchen Zhuge",
                      "orgs": [
                          "KAUST"
                      ]
                  },
                  {
                      "name": "Changsheng Zhao",
                      "orgs": [
                          "Meta"
                      ]
                  },
                  {
                      "name": "Dylan R. Ashley",
                      "orgs": []
                  },
                  {
                      "name": "Wenyi Wang",
                      "orgs": []
                  },
                  {
                      "name": "Dmitrii Khizbullin",
                      "orgs": []
                  },
                  {
                      "name": "Yunyang Xiong",
                      "orgs": []
                  },
                  {
                      "name": "Zechun Liu",
                      "orgs": []
                  },
                  {
                      "name": "Ernie Chang",
                      "orgs": []
                  },
                  {
                      "name": "Raghuraman Krishnamoorthi",
                      "orgs": []
                  },
                  {
                      "name": "Yuandong Tian",
                      "orgs": []
                  },
                  {
                      "name": "Yangyang Shi",
                      "orgs": []
                  },
                  {
                      "name": "Vikas Chandra",
                      "orgs": []
                  },
                  {
                      "name": "Jürgen Schmidhuber",
                      "orgs": []
                  }
              ],
              "tldr": "This paper addresses the challenge of evaluating agentic systems by introducing the Agent-as-a-Judge framework, which uses agentic systems to provide rich intermediate feedback, outperforming LLM-as-a-Judge and matching human evaluators in 90% consensus, while saving 97.72% time and 97.64% cost compared to human evaluation.",
              "arxiv_id": "2410.10934",
              "urls": [
                  "https://arxiv.org/pdf/2410.10934v2"
              ],
              "dates": [
                  "2024-10-14T17:57:02Z"
              ],
              "score": 62.5
          },
          {
              "sources": [
                  "section"
              ],
              "title": "ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis",
              "authors": [
                  {
                      "name": "Congzhi Zhang",
                      "orgs": [
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Zhibin Wang",
                      "orgs": [
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yinchao Ma",
                      "orgs": [
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jiawei Peng",
                      "orgs": [
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Yihan Wang",
                      "orgs": [
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Qiang Zhou",
                      "orgs": [
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Jun Song",
                      "orgs": [
                          "Alibaba Group"
                      ]
                  },
                  {
                      "name": "Bo Zheng",
                      "orgs": [
                          "Alibaba Group"
                      ]
                  }
              ],
              "tldr": "This paper addresses the gap in complex video reasoning for Large Vision-Language Models by introducing ReWatch, a dataset synthesized via a multi-stage agentic pipeline with a Multi-Agent ReAct framework, and ReWatch-R1, a model achieving state-of-the-art performance on five benchmarks through an O\\&R reward mechanism that evaluates both answer correctness and video-grounded reasoning.",
              "arxiv_id": "2509.23652",
              "urls": [
                  "https://arxiv.org/pdf/2509.23652v2"
              ],
              "dates": [
                  "2025-09-28T05:38:16Z"
              ],
              "score": 62.5
          },
          {
              "sources": [
                  "roc"
              ],
              "title": "Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization",
              "authors": [
                  {
                      "name": "Yihong Wu",
                      "orgs": [
                          "Université de Montréal"
                      ]
                  },
                  {
                      "name": "Liheng Ma",
                      "orgs": [
                          "McGill University & Mila - Quebec AI Institute"
                      ]
                  },
                  {
                      "name": "Muzhi Li",
                      "orgs": [
                          "The Chinese University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Jiaming Zhou",
                      "orgs": [
                          "Huawei Noah’s Ark Lab",
                          "Tianjin University"
                      ]
                  },
                  {
                      "name": "Jianye Hao",
                      "orgs": [
                          "Huawei Noah’s Ark Lab",
                          "Tianjin University"
                      ]
                  },
                  {
                      "name": "Ho-fung Leung",
                      "orgs": [
                          "The Chinese University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Irwin King",
                      "orgs": [
                          "The Chinese University of Hong Kong"
                      ]
                  },
                  {
                      "name": "Yingxue Zhang",
                      "orgs": [
                          "Huawei Noah’s Ark Lab"
                      ]
                  },
                  {
                      "name": "Jian-Yun Nie",
                      "orgs": [
                          "Université de Montréal"
                      ]
                  }
              ],
              "tldr": "This paper addresses the challenge of multi-hop question answering by proposing Mujica, a framework combining a planner and worker for subquestion decomposition and resolution, and MyGO, a reinforcement learning method using Maximum Likelihood Estimation for efficient and stable training, demonstrating improved performance across datasets.",
              "arxiv_id": "2505.17086",
              "urls": [
                  "https://arxiv.org/pdf/2505.17086v2"
              ],
              "dates": [
                  "2025-05-20T18:33:03Z"
              ],
              "score": 62.5
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty",
              "authors": [
                  {
                      "name": "Peilin Wu",
                      "orgs": [
                          "Department of Computer Science, The University of Texas at Dallas"
                      ]
                  },
                  {
                      "name": "Mian Zhang",
                      "orgs": [
                          "Department of Computer Science, The University of Texas at Dallas"
                      ]
                  },
                  {
                      "name": "Xinlu Zhang",
                      "orgs": [
                          "Department of Computer Science, University of California, Santa Barbara"
                      ]
                  },
                  {
                      "name": "Xinya Du",
                      "orgs": [
                          "Department of Computer Science, The University of Texas at Dallas"
                      ]
                  },
                  {
                      "name": "Zhiyu Zoey Chen",
                      "orgs": [
                          "Department of Computer Science, The University of Texas at Dallas"
                      ]
                  }
              ],
              "tldr": "This paper addresses sub-optimal search behaviors in agentic RAG systems, aiming to improve efficiency and reliability by reducing uncertainty-driven over- and under-searching; it introduces $\\beta$-GRPO, a reinforcement learning method that rewards high-certainty search decisions, leading to a 4% higher exact match score and fewer search errors in QA benchmarks.",
              "arxiv_id": "2505.17281",
              "urls": [
                  "https://arxiv.org/pdf/2505.17281v2"
              ],
              "dates": [
                  "2025-05-22T20:57:56Z"
              ],
              "score": 61.25
          },
          {
              "sources": [
                  "roc",
                  "section"
              ],
              "title": "LIMI: Less is More for Agency",
              "authors": [
                  {
                      "name": "Yang Xiao",
                      "orgs": []
                  },
                  {
                      "name": "Mohan Jiang",
                      "orgs": []
                  },
                  {
                      "name": "Jie Sun",
                      "orgs": []
                  },
                  {
                      "name": "Keyu Li",
                      "orgs": []
                  },
                  {
                      "name": "Jifan Lin",
                      "orgs": []
                  },
                  {
                      "name": "Yumin Zhuang",
                      "orgs": []
                  },
                  {
                      "name": "Ji Zeng",
                      "orgs": []
                  },
                  {
                      "name": "Shijie Xia",
                      "orgs": []
                  },
                  {
                      "name": "Qishuo Hua",
                      "orgs": []
                  },
                  {
                      "name": "Xuefeng Li",
                      "orgs": []
                  },
                  {
                      "name": "Xiaojie Cai",
                      "orgs": []
                  },
                  {
                      "name": "Tongyu Wang",
                      "orgs": []
                  },
                  {
                      "name": "Yue Zhang",
                      "orgs": []
                  },
                  {
                      "name": "Liming Liu",
                      "orgs": []
                  },
                  {
                      "name": "Xia Wu",
                      "orgs": []
                  },
                  {
                      "name": "Jinlong Hou",
                      "orgs": []
                  },
                  {
                      "name": "Yuan Cheng",
                      "orgs": []
                  },
                  {
                      "name": "Wenjie Li",
                      "orgs": []
                  },
                  {
                      "name": "Xiang Wang",
                      "orgs": []
                  },
                  {
                      "name": "Dequan Wang",
                      "orgs": []
                  },
                  {
                      "name": "Pengfei Liu",
                      "orgs": []
                  }
              ],
              "tldr": "This paper addresses the challenge of developing agentic AI systems that can execute tasks and drive real-world outcomes, challenging the assumption that more data leads to better agency; it introduces LIMI, a method using strategically curated training samples to achieve sophisticated agentic intelligence; LIMI achieves 73.5% on agency benchmarks with 78 samples, outperforming state-of-the-art models and demonstrating 53.7% improvement over models trained on 10,000 samples.",
              "arxiv_id": "2509.17567",
              "urls": [
                  "https://arxiv.org/pdf/2509.17567v2"
              ],
              "dates": [
                  "2025-09-22T10:59:32Z"
              ],
              "score": 61.25
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
              "authors": [
                  {
                      "name": "Davide Paglieri",
                      "orgs": [
                          "AI Centre",
                          "University College London"
                      ]
                  },
                  {
                      "name": "Bartłomiej Cupiał",
                      "orgs": [
                          "IDEAS NCBR",
                          "University of Warsaw"
                      ]
                  },
                  {
                      "name": "Samuel Coward",
                      "orgs": [
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Ulyana Piterbarg",
                      "orgs": [
                          "New York University"
                      ]
                  },
                  {
                      "name": "Maciej Wołczyk",
                      "orgs": [
                          "IDEAS NCBR",
                          "University of Warsaw",
                          "Institute of Mathematics, Polish Academy of Sciences"
                      ]
                  },
                  {
                      "name": "Akbir Khan",
                      "orgs": [
                          "AI Centre",
                          "University College London",
                          "Anthropic"
                      ]
                  },
                  {
                      "name": "Eduardo Pignatelli",
                      "orgs": [
                          "AI Centre",
                          "University College London"
                      ]
                  },
                  {
                      "name": "Łukasz Kuciski",
                      "orgs": [
                          "IDEAS NCBR",
                          "University of Warsaw",
                          "Institute of Mathematics, Polish Academy of Sciences"
                      ]
                  },
                  {
                      "name": "Lerrel Pinto",
                      "orgs": [
                          "New York University"
                      ]
                  },
                  {
                      "name": "Rob Fergus",
                      "orgs": [
                          "New York University"
                      ]
                  },
                  {
                      "name": "Jakob Nicolaus Foerster",
                      "orgs": [
                          "University of Oxford"
                      ]
                  },
                  {
                      "name": "Jack Parker-Holder",
                      "orgs": [
                          "AI Centre",
                          "University College London"
                      ]
                  },
                  {
                      "name": "Tim Rocktäschel",
                      "orgs": [
                          "AI Centre",
                          "University College London"
                      ]
                  }
              ],
              "tldr": "This paper introduces BALROG, a benchmark for evaluating agentic LLM and VLM reasoning in complex games, addressing the gap in assessing long-horizon decision-making and spatial reasoning; it uses six reinforcement learning environments with varying difficulty levels and fine-grained metrics, showing that current models struggle with challenging tasks like NetHack and exhibit poor vision-based decision-making; findings highlight significant performance drops in visual settings and a knowing-doing gap in model capabilities.",
              "arxiv_id": "2411.13543",
              "urls": [
                  "https://arxiv.org/pdf/2411.13543v2"
              ],
              "dates": [
                  "2024-11-20T18:54:32Z"
              ],
              "score": 61.25
          },
          {
              "sources": [
                  "metadata"
              ],
              "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
              "authors": [
                  {
                      "name": "GLM-4.5 Team",
                      "orgs": [
                          "Zhipu AI",
                          "Tsinghua University"
                      ]
                  }
              ],
              "tldr": "This paper introduces GLM-4.5, a large language model addressing agentic, reasoning, and coding tasks by unifying these capabilities in a single open-source model, which is crucial for advancing general problem-solving AI; it proposes a hybrid reasoning MoE architecture with 355B parameters and achieves 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified, outperforming competitors with fewer parameters.",
              "arxiv_id": "2508.06471",
              "urls": [
                  "https://arxiv.org/pdf/2508.06471v1"
              ],
              "dates": [
                  "2025-08-08T17:21:06Z"
              ],
              "score": 60.0
          },
          {
              "sources": [
                  "introduction"
              ],
              "title": "The Measurement Imbalance in Agentic AI Evaluation Undermines Industry Productivity Claims",
              "authors": [
                  {
                      "name": "Kiana Jafari Meimandi",
                      "orgs": [
                          "Stanford University"
                      ]
                  },
                  {
                      "name": "Gabriela Aranguiz-Dias",
                      "orgs": [
                          "Stanford University"
                      ]
                  },
                  {
                      "name": "Grace Ra Kim",
                      "orgs": [
                          "Stanford University"
                      ]
                  },
                  {
                      "name": "Lana Saadeddin",
                      "orgs": [
                          "Montclair State University"
                      ]
                  },
                  {
                      "name": "Mykel J. Kochenderfer",
                      "orgs": [
                          "Stanford University"
                      ]
                  }
              ],
              "tldr": "This survey examines agentic AI evaluation imbalances, highlighting a dominance of technical metrics (83%) over human, safety, and economic assessments (30-53%), and proposes a four-axis model to address these gaps for more realistic deployment outcomes.",
              "arxiv_id": "2506.02064",
              "urls": [
                  "https://arxiv.org/pdf/2506.02064v3"
              ],
              "dates": [
                  "2025-06-01T19:45:04Z"
              ],
              "score": 60.0
          }
      ]
  }
]